diff --git a/kernel/nvidia-uvm/nvidia-uvm-sources.Kbuild b/kernel/nvidia-uvm/nvidia-uvm-sources.Kbuild
index 0680f82..740bbba 100644
--- a/kernel/nvidia-uvm/nvidia-uvm-sources.Kbuild
+++ b/kernel/nvidia-uvm/nvidia-uvm-sources.Kbuild
@@ -98,3 +98,4 @@ NVIDIA_UVM_SOURCES += nvidia-uvm/uvm8_peer_identity_mappings_test.c
 NVIDIA_UVM_SOURCES += nvidia-uvm/uvm8_va_block_test.c
 NVIDIA_UVM_SOURCES += nvidia-uvm/uvm8_range_group_tree_test.c
 NVIDIA_UVM_SOURCES += nvidia-uvm/uvm8_thread_context_test.c
+NVIDIA_UVM_SOURCES += nvidia-uvm/uvm8_nvmgpu.c
diff --git a/kernel/nvidia-uvm/uvm8.c b/kernel/nvidia-uvm/uvm8.c
index 426c047..6f24998 100644
--- a/kernel/nvidia-uvm/uvm8.c
+++ b/kernel/nvidia-uvm/uvm8.c
@@ -35,6 +35,7 @@
 #include "uvm_linux_ioctl.h"
 #include "uvm8_hmm.h"
 #include "uvm8_mem.h"
+#include "uvm8_nvmgpu.h"
 
 #define NVIDIA_UVM_DEVICE_NAME          "nvidia-uvm"
 
@@ -194,6 +195,15 @@ static void uvm_destroy_vma_managed(struct vm_area_struct *vma, bool is_uvm_tear
         UVM_ASSERT(uvm_va_range_vma(va_range) == vma);
         UVM_ASSERT(va_range->node.start >= vma->vm_start);
         UVM_ASSERT(va_range->node.end   <  vma->vm_end);
+
+        if (uvm_nvmgpu_is_managed(va_range)) {
+            if ((va_range->nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_WRITE) 
+                && !(va_range->nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_VOLATILE)) {
+                uvm_nvmgpu_flush(va_range);
+            }
+            uvm_nvmgpu_unregister_va_range(va_range);
+        }
+
         size += uvm_va_range_size(va_range);
         if (is_uvm_teardown)
             uvm_va_range_zombify(va_range);
@@ -555,6 +565,9 @@ static vm_fault_t uvm_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
         status = uvm_va_block_cpu_fault(va_block, fault_addr, is_write, service_context);
     } while (status == NV_WARN_MORE_PROCESSING_REQUIRED);
 
+    if (uvm_nvmgpu_is_va_space_initialized(va_space) && uvm_nvmgpu_has_to_reclaim_blocks(va_space->nvmgpu_va_space))
+        uvm_nvmgpu_reduce_memory_consumption(va_space);
+
     if (status != NV_OK) {
         UvmEventFatalReason reason;
 
@@ -935,6 +948,10 @@ static long uvm_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
         UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_CLEAN_UP_ZOMBIE_RESOURCES,      uvm_api_clean_up_zombie_resources);
         UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_POPULATE_PAGEABLE,              uvm_api_populate_pageable);
         UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_VALIDATE_VA_RANGE,              uvm_api_validate_va_range);
+
+        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_NVMGPU_INITIALIZE,              uvm_api_nvmgpu_initialize);
+        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_NVMGPU_REGISTER_FILE_VA_SPACE,  uvm_api_nvmgpu_register_file_va_space);
+        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_NVMGPU_REMAP,                   uvm_api_nvmgpu_remap);
     }
 
     // Try the test ioctls if none of the above matched
diff --git a/kernel/nvidia-uvm/uvm8_api.h b/kernel/nvidia-uvm/uvm8_api.h
index 09a2562..0ec8968 100644
--- a/kernel/nvidia-uvm/uvm8_api.h
+++ b/kernel/nvidia-uvm/uvm8_api.h
@@ -245,4 +245,8 @@ NV_STATUS uvm_api_migrate_range_group(UVM_MIGRATE_RANGE_GROUP_PARAMS *params, st
 NV_STATUS uvm_api_alloc_semaphore_pool(UVM_ALLOC_SEMAPHORE_POOL_PARAMS *params, struct file *filp);
 NV_STATUS uvm_api_populate_pageable(const UVM_POPULATE_PAGEABLE_PARAMS *params, struct file *filp);
 
+NV_STATUS uvm_api_nvmgpu_initialize(UVM_NVMGPU_INITIALIZE_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_nvmgpu_register_file_va_space(UVM_NVMGPU_REGISTER_FILE_VA_SPACE_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_nvmgpu_remap(UVM_NVMGPU_REMAP_PARAMS *params, struct file *filp);
+
 #endif // __UVM8_API_H__
diff --git a/kernel/nvidia-uvm/uvm8_forward_decl.h b/kernel/nvidia-uvm/uvm8_forward_decl.h
index e873a0a..ff9e441 100644
--- a/kernel/nvidia-uvm/uvm8_forward_decl.h
+++ b/kernel/nvidia-uvm/uvm8_forward_decl.h
@@ -89,4 +89,7 @@ typedef struct uvm_pmm_sysmem_mappings_struct uvm_pmm_sysmem_mappings_t;
 
 typedef struct uvm_reverse_map_struct uvm_reverse_map_t;
 
+typedef struct uvm_nvmgpu_va_space_struct uvm_nvmgpu_va_space_t;
+typedef struct uvm_nvmgpu_va_range_struct uvm_nvmgpu_va_range_t;
+
 #endif //__UVM8_FORWARD_DECL_H__
diff --git a/kernel/nvidia-uvm/uvm8_gpu.c b/kernel/nvidia-uvm/uvm8_gpu.c
index 303d89a..7ad67f0 100644
--- a/kernel/nvidia-uvm/uvm8_gpu.c
+++ b/kernel/nvidia-uvm/uvm8_gpu.c
@@ -2825,3 +2825,4 @@ NV_STATUS uvm8_test_get_gpu_time(UVM_TEST_GET_GPU_TIME_PARAMS *params, struct fi
 
     return status;
 }
+
diff --git a/kernel/nvidia-uvm/uvm8_gpu_replayable_faults.c b/kernel/nvidia-uvm/uvm8_gpu_replayable_faults.c
index fad4ac3..4addb83 100644
--- a/kernel/nvidia-uvm/uvm8_gpu_replayable_faults.c
+++ b/kernel/nvidia-uvm/uvm8_gpu_replayable_faults.c
@@ -39,6 +39,7 @@
 #include "uvm8_ats_ibm.h"
 #include "uvm8_ats_faults.h"
 #include "uvm8_test.h"
+#include "uvm8_nvmgpu.h"
 
 // TODO: Bug 1881601: [uvm] Add fault handling overview for replayable and
 // non-replayable faults
@@ -1586,6 +1587,9 @@ static NV_STATUS service_fault_batch(uvm_gpu_t *gpu,
             status = invalidate_status;
     }
 
+    if (uvm_nvmgpu_is_va_space_initialized(va_space) && uvm_nvmgpu_has_to_reclaim_blocks(va_space->nvmgpu_va_space))
+        uvm_nvmgpu_reduce_memory_consumption(va_space);
+
 fail:
     if (va_space != NULL) {
         uvm_va_space_up_read(va_space);
diff --git a/kernel/nvidia-uvm/uvm8_lock.c b/kernel/nvidia-uvm/uvm8_lock.c
index 6ae8400..a5fb194 100644
--- a/kernel/nvidia-uvm/uvm8_lock.c
+++ b/kernel/nvidia-uvm/uvm8_lock.c
@@ -27,7 +27,7 @@
 
 const char *uvm_lock_order_to_string(uvm_lock_order_t lock_order)
 {
-    BUILD_BUG_ON(UVM_LOCK_ORDER_COUNT != 26);
+    BUILD_BUG_ON(UVM_LOCK_ORDER_COUNT != 27);
 
     switch (lock_order) {
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_INVALID);
@@ -43,6 +43,7 @@ const char *uvm_lock_order_to_string(uvm_lock_order_t lock_order)
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_GPU_SEMAPHORE_POOL);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_RM_API);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_RM_GPUS);
+        UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_NVMGPU);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_VA_BLOCK);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_PAGE_TREE);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_SWIZZLE_STAGING);
diff --git a/kernel/nvidia-uvm/uvm8_lock.h b/kernel/nvidia-uvm/uvm8_lock.h
index f2531fa..fc856ef 100644
--- a/kernel/nvidia-uvm/uvm8_lock.h
+++ b/kernel/nvidia-uvm/uvm8_lock.h
@@ -390,6 +390,7 @@ typedef enum
     UVM_LOCK_ORDER_GPU_SEMAPHORE_POOL,
     UVM_LOCK_ORDER_RM_API,
     UVM_LOCK_ORDER_RM_GPUS,
+    UVM_LOCK_ORDER_NVMGPU,
     UVM_LOCK_ORDER_VA_BLOCK,
     UVM_LOCK_ORDER_PAGE_TREE,
     UVM_LOCK_ORDER_SWIZZLE_STAGING,
@@ -782,6 +783,18 @@ static void uvm_mutex_init(uvm_mutex_t *mutex, uvm_lock_order_t lock_order)
         uvm_assert_mutex_locked(_mutex);                        \
     })
 
+#define uvm_mutex_trylock(mutex) ({                                                         \
+        typeof(mutex) _mutex = (mutex);                                                     \
+        int locked;                                                                         \
+        uvm_record_lock(_mutex, UVM_LOCK_FLAGS_MODE_EXCLUSIVE | UVM_LOCK_FLAGS_TRYLOCK);    \
+        locked = mutex_trylock(&_mutex->m);                                                 \
+        if (locked == 0)                                                                    \
+            uvm_record_unlock(_mutex, UVM_LOCK_FLAGS_MODE_EXCLUSIVE);                       \
+        else                                                                                \
+            uvm_assert_mutex_locked(_mutex);                                                \
+        locked;                                                                             \
+    })
+
 // Lock w/o any tracking. This should be extremely rare and *_no_tracking
 // helpers will be added only as needed.
 #define uvm_mutex_lock_no_tracking(mutex) mutex_lock(&(mutex)->m)
diff --git a/kernel/nvidia-uvm/uvm8_nvmgpu.c b/kernel/nvidia-uvm/uvm8_nvmgpu.c
new file mode 100644
index 0000000..d91ec32
--- /dev/null
+++ b/kernel/nvidia-uvm/uvm8_nvmgpu.c
@@ -0,0 +1,2082 @@
+#include <linux/syscalls.h>
+#include <linux/delay.h>
+#include <linux/aio.h>
+#include <linux/swap.h>
+#include <linux/writeback.h>
+#include <linux/fs.h>
+#include <linux/fsnotify.h>
+#include <linux/sched/xacct.h>
+#include <linux/backing-dev.h>
+#include <linux/pci-p2pdma.h>
+#include <linux/genalloc.h>
+
+#include "nv_uvm_interface.h"
+#include "uvm8_api.h"
+#include "uvm8_channel.h"
+#include "uvm8_global.h"
+#include "uvm8_gpu.h"
+#include "uvm8_gpu_semaphore.h"
+#include "uvm8_hal.h"
+#include "uvm8_procfs.h"
+#include "uvm8_pmm_gpu.h"
+#include "uvm8_va_space.h"
+#include "uvm8_gpu_replayable_faults.h"
+#include "uvm8_user_channel.h"
+#include "uvm8_perf_events.h"
+#include "uvm_common.h"
+#include "ctrl2080mc.h"
+#include "nv-kthread-q.h"
+#include "uvm_linux.h"
+#include "uvm_common.h"
+#include "uvm8_va_range.h"
+#include "uvm8_va_block.h"
+#include "uvm8_hal_types.h"
+#include "uvm8_kvmalloc.h"
+#include "uvm8_push.h"
+#include "uvm8_perf_thrashing.h"
+#include "uvm8_perf_prefetch.h"
+#include "uvm8_mem.h"
+#include "uvm8_rm_mem.h"
+#include "uvm8_nvmgpu.h"
+
+#define MIN(x,y) (x < y ? x : y)
+
+static void *fsdata_array[PAGES_PER_UVM_VA_BLOCK];
+
+// Copied from drivers/pci/p2pdma.c
+struct pci_p2pdma {
+	struct gen_pool *pool;
+	bool p2pmem_published;
+	struct xarray map_types;
+};
+
+// -----------------------------------------------------------------------------------------------------
+// API
+// -----------------------------------------------------------------------------------------------------
+
+NV_STATUS uvm_api_nvmgpu_initialize(UVM_NVMGPU_INITIALIZE_PARAMS *params, struct file *filp)
+{
+    uvm_va_space_t *va_space = uvm_va_space_get(filp);
+    return uvm_nvmgpu_initialize(va_space, params);
+}
+
+NV_STATUS uvm_api_nvmgpu_register_file_va_space(UVM_NVMGPU_REGISTER_FILE_VA_SPACE_PARAMS *params, struct file *filp)
+{
+    uvm_va_space_t *va_space = uvm_va_space_get(filp);
+    return uvm_nvmgpu_register_file_va_space(va_space, params);
+}
+
+NV_STATUS uvm_api_nvmgpu_remap(UVM_NVMGPU_REMAP_PARAMS *params, struct file *filp)
+{
+    uvm_va_space_t *va_space = uvm_va_space_get(filp);
+    return uvm_nvmgpu_remap(va_space, params);
+}
+
+// -----------------------------------------------------------------------------------------------------
+
+static NV_STATUS nvmgpu_gdirect_unregister_pfn(uvm_gpu_t *gpu, unsigned long pfn)
+{
+    struct pci_dev *pdev = gpu->pci_dev;
+
+    struct dev_pagemap *pgmap = NULL;
+
+    pgmap = get_dev_pagemap(pfn, NULL);
+    // This PFN has not been registered. So exit.
+    if (!pgmap)
+        return NV_OK;
+
+    put_dev_pagemap(pgmap);
+
+    // Pool has been created. Remove it and replace with a new one.
+    if (pdev->p2pdma && pdev->p2pdma->pool)
+    {
+        gen_pool_destroy(pdev->p2pdma->pool);
+        pdev->p2pdma->pool = gen_pool_create(PAGE_SHIFT, dev_to_node(&pdev->dev));
+        if (!pdev->p2pdma->pool)
+        {
+            printk(KERN_DEBUG "Cannot allocate p2pdma->pool\n");
+            return NV_ERR_NO_MEMORY;
+        }
+    }
+
+    devm_memunmap_pages(&pdev->dev, pgmap);
+    devm_kfree(&pdev->dev, pgmap);
+
+    return NV_OK;
+}
+
+static NV_STATUS uvm_nvmgpu_gdirect_initialize(uvm_nvmgpu_gdirect_t *gdirect, uvm_gpu_t *gpu, unsigned int num_groups)
+{
+    NV_STATUS ret = NV_OK;
+
+    int i;
+
+    uvm_rm_mem_t *rm_mem = NULL;
+
+    void *cpu_va = NULL;
+    NvU64 gpu_va = 0;
+    NvU64 aligned_gpu_va = 0;
+
+    unsigned long original_pfn = 0;
+    unsigned long pfn = 0;
+
+    u64 bar1_offset = 0;
+
+    struct dev_pagemap *conflict_pgmap = NULL;
+    struct pci_dev *pdev_client = NULL;
+
+    void *p2pmem_va = NULL;
+
+    struct bio_vec *bvecs = NULL;
+
+    size_t p2pmem_size;
+    unsigned int p2pmem_num_pages;
+    size_t rm_mem_alloc_size;
+
+    unsigned int num_read_pages_per_group;
+    unsigned int num_write_pages_per_group;
+
+    uvm_nvmgpu_gdirect_buffer_t *read_buffers = NULL;
+    uvm_nvmgpu_gdirect_buffer_t *write_buffer = NULL;
+
+    gdirect->p2pmem_support = false;
+
+    UVM_ASSERT(num_groups > 0);
+
+    // num_groups read buffers + 1 write buffer
+    p2pmem_size = UVM_VA_BLOCK_SIZE * (UVM_NVMGPU_GDIRECT_BUF_NUM_BLOCKS * num_groups + 1);
+
+    // Size has to be divisible by SUBSECTION's size.
+    p2pmem_size = (p2pmem_size + (PAGES_PER_SUBSECTION << PAGE_SHIFT) - 1) & ~((size_t)((PAGES_PER_SUBSECTION << PAGE_SHIFT) - 1));
+
+    p2pmem_num_pages = p2pmem_size / PAGE_SIZE;
+
+    num_write_pages_per_group = UVM_VA_BLOCK_SIZE / PAGE_SIZE;
+    num_read_pages_per_group  = (p2pmem_num_pages - num_write_pages_per_group) / num_groups;
+
+    // We need to align PFN later, so allocate more memory for alignment.
+    rm_mem_alloc_size = p2pmem_size + (PAGES_PER_SUBSECTION << PAGE_SHIFT) - 1;
+
+    // Allocate rm_mem on GPU to be used as GPUDirect buffer.
+    ret = uvm_rm_mem_alloc_and_map_cpu(gpu, UVM_RM_MEM_TYPE_GPU, rm_mem_alloc_size, &rm_mem);
+    if (ret != NV_OK) 
+    {
+        printk(KERN_DEBUG "error in uvm_rm_mem_alloc_and_map_cpu\n");
+        goto err;
+    }
+
+    // Find BAR1 PFN from mapped VA.
+    cpu_va = uvm_rm_mem_get_cpu_va(rm_mem);
+    original_pfn = vmalloc_to_pfn(cpu_va);
+
+    // P2PDMA requires alignment, so do it.
+    pfn = SUBSECTION_ALIGN_UP(original_pfn);
+
+    // Calculate the offset w.r.t. the GPU BAR1 start address.
+    bar1_offset = PFN_PHYS(pfn) - pci_resource_start(gpu->pci_dev, 1);
+
+    // Record aligned GPU VA.
+    gpu_va = uvm_rm_mem_get_gpu_va(rm_mem, gpu);
+    aligned_gpu_va = gpu_va + ((pfn - original_pfn) << PAGE_SHIFT);
+
+    // If this PFN has already been registered. Try to unregister it.
+    conflict_pgmap = get_dev_pagemap(pfn, NULL);
+    if (conflict_pgmap)
+    {
+        put_dev_pagemap(conflict_pgmap);
+        nvmgpu_gdirect_unregister_pfn(gpu, pfn);
+    }
+
+    // Register this PFN with P2PMEM.
+    gdirect->p2pmem_support = (pci_p2pdma_add_resource(gpu->pci_dev, 1, p2pmem_size, bar1_offset) == 0);
+
+    // P2PMEM is not supported. Revert the operations.
+    if (!gdirect->p2pmem_support)
+    {
+        ret = NV_ERR_NOT_SUPPORTED;
+        printk(KERN_DEBUG "p2pmem is not supported\n");
+        goto err;
+    }
+
+    // Publish this P2PMEM.
+    pci_p2pmem_publish(gpu->pci_dev, true);
+
+    // Try to register other PCI devices as clients to this GPU.
+    // TODO: We should record which devices can do P2P.
+    pdev_client = NULL;
+    while ((pdev_client = pci_get_device(PCI_ANY_ID, PCI_ANY_ID, pdev_client)))
+        pci_p2pdma_distance(gpu->pci_dev, &pdev_client->dev, true);
+
+    bvecs = uvm_kvmalloc(sizeof(struct bio_vec) * p2pmem_num_pages);
+    if (!bvecs)
+    {
+        ret = NV_ERR_NO_MEMORY;
+        printk(KERN_DEBUG "error in uvm_kvmalloc\n");
+        goto err;
+    }
+
+    write_buffer = uvm_kvmalloc_zero(sizeof(uvm_nvmgpu_gdirect_buffer_t));
+    if (!write_buffer)
+    {
+        ret = NV_ERR_NO_MEMORY;
+        printk(KERN_DEBUG "error in uvm_kvmalloc\n");
+        goto err;
+    }
+
+    read_buffers = uvm_kvmalloc_zero(sizeof(uvm_nvmgpu_gdirect_buffer_t) * num_groups);
+    if (!read_buffers)
+    {
+        ret = NV_ERR_NO_MEMORY;
+        printk(KERN_DEBUG "error in uvm_kvmalloc\n");
+        goto err;
+    }
+
+    // Lock this p2pmem for GPUDirect.
+    p2pmem_va = pci_alloc_p2pmem(gpu->pci_dev, p2pmem_size);
+    if (!p2pmem_va)
+    {
+        ret = NV_ERR_NO_MEMORY;
+        printk(KERN_DEBUG "error in pci_alloc_p2pmem\n");
+        goto err;
+    }
+
+    for (i = 0; i < p2pmem_num_pages; ++i)
+    {
+        bvecs[i].bv_page = virt_to_page((void *)((NvU64)p2pmem_va + (PAGE_SIZE * i)));
+        bvecs[i].bv_len = PAGE_SIZE;
+        bvecs[i].bv_offset = 0;
+    }
+
+    uvm_mutex_init(&write_buffer->lock, UVM_LOCK_ORDER_NVMGPU);
+    write_buffer->gdirect = gdirect;
+    write_buffer->bvecs = bvecs;
+    write_buffer->num_pages = num_write_pages_per_group;
+    write_buffer->aligned_rm_gpu_va = aligned_gpu_va;
+
+    for (i = 0; i < num_groups; ++i)
+    {
+        unsigned int page_idx = num_write_pages_per_group + i * num_read_pages_per_group;
+        uvm_mutex_init(&read_buffers[i].lock, UVM_LOCK_ORDER_NVMGPU);
+        read_buffers[i].gdirect = gdirect;
+        read_buffers[i].bvecs = &bvecs[page_idx];
+        read_buffers[i].num_pages = num_read_pages_per_group;
+        read_buffers[i].aligned_rm_gpu_va = aligned_gpu_va + page_idx * PAGE_SIZE;
+    }
+
+    gdirect->gpu = gpu;
+
+    gdirect->rm_mem = rm_mem;
+    gdirect->aligned_rm_gpu_va = aligned_gpu_va;
+
+    gdirect->p2pmem_pfn = pfn;
+    gdirect->p2pmem_va = p2pmem_va;
+    gdirect->p2pmem_size = p2pmem_size;
+    gdirect->p2pmem_num_pages = p2pmem_num_pages;
+
+    gdirect->bvecs = bvecs;
+
+    gdirect->num_read_buffers = num_groups;
+    gdirect->read_buffers = read_buffers;
+    gdirect->write_buffer = write_buffer;
+
+    return NV_OK;
+
+err:
+    pci_p2pmem_publish(gpu->pci_dev, false);
+
+    if (write_buffer)
+        uvm_kvfree(write_buffer);
+
+    if (read_buffers)
+        uvm_kvfree(read_buffers);
+
+    if (bvecs)
+        uvm_kvfree(bvecs);
+
+    if (p2pmem_va)
+        pci_free_p2pmem(gpu->pci_dev, p2pmem_va, p2pmem_size);
+
+    if (pfn)
+    {
+        pci_p2pmem_publish(gpu->pci_dev, false);
+        nvmgpu_gdirect_unregister_pfn(gpu, pfn);
+    }
+
+    if (rm_mem)
+        uvm_rm_mem_free(rm_mem);
+    
+    gdirect->p2pmem_support = false;
+
+    return ret;
+}
+
+static void uvm_nvmgpu_gdirect_deinitialize(uvm_nvmgpu_gdirect_t *gdirect)
+{
+    if (gdirect->p2pmem_support)
+    {
+        if (gdirect->write_buffer)
+            uvm_kvfree(gdirect->write_buffer);
+
+        if (gdirect->read_buffers)
+            uvm_kvfree(gdirect->read_buffers);
+
+        if (gdirect->bvecs)
+            uvm_kvfree(gdirect->bvecs);
+
+        if (gdirect->p2pmem_va)
+        {
+            pci_p2pmem_publish(gdirect->gpu->pci_dev, false);
+            pci_free_p2pmem(gdirect->gpu->pci_dev, gdirect->p2pmem_va, gdirect->p2pmem_size);
+        }
+
+        if (gdirect->p2pmem_pfn)
+            nvmgpu_gdirect_unregister_pfn(gdirect->gpu, gdirect->p2pmem_pfn);
+
+        if (gdirect->rm_mem)
+            uvm_rm_mem_free(gdirect->rm_mem);
+    }
+    gdirect->p2pmem_support = false;
+}
+
+/**
+ * Initialize the NVMGPU module. This function has to be called once per
+ * va_space. It must be called before calling
+ * "uvm_nvmgpu_register_file_va_space"
+ *
+ * @param va_space: va_space to be initialized this module with.
+ * @param params: initialization parameters.
+ *
+ * @return: NV_ERR_INVALID_OPERATION if `va_space` has been initialized already,
+ * otherwise NV_OK.
+ */
+NV_STATUS uvm_nvmgpu_initialize(uvm_va_space_t *va_space, UVM_NVMGPU_INITIALIZE_PARAMS *params)
+{
+    uvm_nvmgpu_va_space_t *nvmgpu_va_space = NULL;
+
+    uvm_gpu_id_t gpu_id;
+    uvm_gpu_t *gpu;
+
+    uvm_nvmgpu_gdirect_t *gdirect;
+    unsigned int gdirect_num_groups = params->gdirect_num_groups;
+
+    bool gdirect_support = false;
+
+    if (!uvm_nvmgpu_is_va_space_initialized(va_space))
+    {
+        nvmgpu_va_space = uvm_kvmalloc_zero(sizeof(uvm_nvmgpu_va_space_t));
+        if (!nvmgpu_va_space)
+            return NV_ERR_NO_MEMORY;
+
+        INIT_LIST_HEAD(&nvmgpu_va_space->lru_head);
+        uvm_mutex_init(&nvmgpu_va_space->lock, UVM_LOCK_ORDER_NVMGPU);
+        nvmgpu_va_space->trash_nr_blocks = params->trash_nr_blocks;
+        nvmgpu_va_space->trash_reserved_nr_pages = params->trash_reserved_nr_pages;
+        nvmgpu_va_space->flags = params->flags;
+
+        if (gdirect_num_groups > 0)
+        {
+            for_each_gpu_id_in_mask(gpu_id, &va_space->registered_gpus)
+            {
+                gdirect = &nvmgpu_va_space->gdirect_table[uvm_id_gpu_index(gpu_id)];
+                gpu = uvm_va_space_get_gpu(va_space, gpu_id);
+                
+                if (uvm_nvmgpu_gdirect_initialize(gdirect, gpu, gdirect_num_groups) != NV_OK)
+                {
+                    gdirect_support = false;
+                    break;
+                }
+                gdirect_support = true;
+            }
+        }
+
+        printk(KERN_DEBUG "nvmgpu: GPUDirect support=%s\n", gdirect_support ? "yes" : "no");
+        nvmgpu_va_space->gdirect_support = gdirect_support;
+
+        nvmgpu_va_space->is_initailized = true;
+
+        va_space->nvmgpu_va_space = nvmgpu_va_space;
+
+        return NV_OK;
+    }
+
+    return NV_ERR_INVALID_OPERATION;
+}
+
+/**
+ * Deinitialize nvmgpu associated with this va_space
+ *
+ * @param va_space: va_space to deinitialize nvmgpu
+ */
+void uvm_nvmgpu_deinitialize(uvm_va_space_t *va_space)
+{
+    if (va_space->nvmgpu_va_space)
+    {
+        uvm_gpu_id_t gpu_id;
+        uvm_nvmgpu_gdirect_t *gdirect;
+
+        #if UVM_NVMGPU_PROFILING
+        uvm_nvmgpu_va_space_t *nvmgpu_va_space = va_space->nvmgpu_va_space;
+
+        printk(KERN_DEBUG "uvm_nvmgpu: profiling: header: dio_read_file(ns),dio_read_memcopy(ns),pc_read_file(ns),pc_read_memcopy(ns),populate_gpu_pages(ns)\n");
+        printk(
+            KERN_DEBUG "uvm_nvmgpu: profiling: data: %llu,%llu,%llu,%llu,%llu\n", 
+            nvmgpu_va_space->time.dio_read_file,
+            nvmgpu_va_space->time.dio_read_memcopy,
+            nvmgpu_va_space->time.pc_read_file,
+            nvmgpu_va_space->time.pc_read_memcopy,
+            nvmgpu_va_space->time.populate_gpu_pages
+        );
+        #endif
+
+        for_each_gpu_id_in_mask(gpu_id, &va_space->registered_gpus)
+        {
+            gdirect = &va_space->nvmgpu_va_space->gdirect_table[uvm_id_gpu_index(gpu_id)];
+            uvm_nvmgpu_gdirect_deinitialize(gdirect);
+        }
+
+        uvm_kvfree(va_space->nvmgpu_va_space);
+        va_space->nvmgpu_va_space = NULL;
+    }
+}
+
+/**
+ * Register a file to this `va_space`.
+ * NVMGPU will start tracking this UVM region if this function return success.
+ *
+ * @param va_space: va_space to register the file to.
+ *
+ * @param params: register parameters containing info about the file, size, etc.
+ *
+ * @return: NV_OK on success, NV_ERR_* otherwise.
+ */
+NV_STATUS uvm_nvmgpu_register_file_va_space(uvm_va_space_t *va_space, UVM_NVMGPU_REGISTER_FILE_VA_SPACE_PARAMS *params)
+{
+    NV_STATUS ret = NV_OK;
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = NULL;
+    uvm_nvmgpu_va_space_t *nvmgpu_va_space;
+
+    uvm_va_range_t *va_range = uvm_va_range_find(va_space, params->uvm_addr);
+    NvU64 expected_start_addr = params->uvm_addr;
+    NvU64 expected_end_addr = expected_start_addr + params->size - 1;
+
+    size_t max_nr_blocks;
+
+    // Make sure that uvm_nvmgpu_initialize is called before this function.
+    if (!uvm_nvmgpu_is_va_space_initialized(va_space))
+    {
+        printk(KERN_DEBUG "Error: Call uvm_nvmgpu_register_file_va_space before uvm_nvmgpu_initialize\n");
+        return NV_ERR_INVALID_OPERATION;
+    }
+
+    nvmgpu_va_space = va_space->nvmgpu_va_space;
+
+    // Find va_range associated with the specified UVM address. Might fail if
+    // the library does not call cudaMallocaManaged before calling this
+    // function.
+    if (!va_range || va_range->node.start != expected_start_addr) 
+    {
+        printk(KERN_DEBUG "Cannot find uvm range 0x%llx - 0x%llx\n", expected_start_addr, expected_end_addr);
+        if (va_range)
+            printk(KERN_DEBUG "Closet uvm range 0x%llx - 0x%llx\n", va_range->node.start, va_range->node.end);
+        return NV_ERR_OPERATING_SYSTEM;
+    }
+
+    // Check compatibility if request DIRECT flag.
+    if (params->flags & UVM_NVMGPU_FLAG_DIRECT)
+    {
+        if (params->flags & ~(UVM_NVMGPU_FLAG_DIRECT | UVM_NVMGPU_FLAG_READ | UVM_NVMGPU_FLAG_WRITE))
+        {
+            printk(KERN_DEBUG "DIRECT flag can be used with READ and/or WRITE flags only\n");
+            ret = NV_ERR_NOT_SUPPORTED;
+            goto error;
+        }
+
+        if (!nvmgpu_va_space->gdirect_support)
+        {
+            printk(KERN_DEBUG "GPUDirect is not supported.\n");
+            ret = NV_ERR_NOT_SUPPORTED;
+            goto error;
+        }
+    }
+
+    nvmgpu_va_range = uvm_kvmalloc_zero(sizeof(uvm_nvmgpu_va_range_t));
+
+    // Get the struct file from the input file descriptor.
+    if ((nvmgpu_va_range->filp = fget(params->backing_fd)) == NULL) 
+    {
+        printk(KERN_DEBUG "Cannot find the backing fd: %d\n", params->backing_fd);
+        ret = NV_ERR_OPERATING_SYSTEM;
+        goto error;
+    }
+
+    // Record the flags and the file size.
+    nvmgpu_va_range->flags = params->flags;
+    nvmgpu_va_range->size = params->size;
+
+    // Calculate the number of blocks associated with this UVM range.
+    max_nr_blocks = uvm_va_range_num_blocks(va_range);
+
+    // Allocate the bitmap to tell which blocks have dirty data on the file.
+    nvmgpu_va_range->is_file_dirty_bitmaps = uvm_kvmalloc_zero(sizeof(unsigned long) * BITS_TO_LONGS(max_nr_blocks));
+    if (!nvmgpu_va_range->is_file_dirty_bitmaps) {
+        ret = NV_ERR_NO_MEMORY;
+        goto error;
+    }
+
+    // Allocate the bitmap to tell which blocks have data cached on the host.
+    nvmgpu_va_range->has_data_bitmaps = uvm_kvmalloc_zero(sizeof(unsigned long) * BITS_TO_LONGS(max_nr_blocks));
+    if (!nvmgpu_va_range->has_data_bitmaps) {
+        ret = NV_ERR_NO_MEMORY;
+        goto error;
+    }
+
+    if ((nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_VOLATILE)
+        || (nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_USEHOSTBUF)
+    ) {
+        nvmgpu_va_range->iov = uvm_kvmalloc(sizeof(struct iovec) * PAGES_PER_UVM_VA_BLOCK);
+        if (!nvmgpu_va_range->iov) {
+            ret = NV_ERR_NO_MEMORY;
+            goto error;
+        }
+    }
+
+    va_range->nvmgpu_va_range = nvmgpu_va_range;
+
+    return NV_OK; 
+
+error:
+    // Found an error. Free allocated memory before go out.
+    if (nvmgpu_va_range)
+    {
+        if (nvmgpu_va_range->iov)
+            uvm_kvfree(nvmgpu_va_range->iov);
+
+        if (nvmgpu_va_range->has_data_bitmaps)
+            uvm_kvfree(nvmgpu_va_range->has_data_bitmaps);
+
+        if (nvmgpu_va_range->is_file_dirty_bitmaps)
+            uvm_kvfree(nvmgpu_va_range->is_file_dirty_bitmaps);
+
+        if (nvmgpu_va_range->filp)
+            fput(nvmgpu_va_range->filp);
+
+        uvm_kvfree(nvmgpu_va_range);
+    }
+    return ret;
+}
+
+NV_STATUS uvm_nvmgpu_remap(uvm_va_space_t *va_space, UVM_NVMGPU_REMAP_PARAMS *params)
+{
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range;
+    uvm_va_block_t *va_block, *va_block_next;
+    uvm_nvmgpu_va_space_t *nvmgpu_va_space = va_space->nvmgpu_va_space;
+
+    uvm_va_range_t *va_range = uvm_va_range_find(va_space, params->uvm_addr);
+    NvU64 expected_start_addr = params->uvm_addr;
+
+    // Make sure that uvm_nvmgpu_initialize is called before this function.
+    if (!uvm_nvmgpu_is_va_space_initialized(va_space))
+    {
+        printk(KERN_DEBUG "Error: Call uvm_nvmgpu_remap before uvm_nvmgpu_initialize\n");
+        return NV_ERR_INVALID_OPERATION;
+    }
+
+    if (!va_range || va_range->node.start != expected_start_addr) {
+        printk(KERN_DEBUG "Cannot find uvm whose address starts from 0x%llx\n", expected_start_addr);
+        if (va_range)
+            printk(KERN_DEBUG "Closet uvm range 0x%llx - 0x%llx\n", va_range->node.start, va_range->node.end);
+        return NV_ERR_OPERATING_SYSTEM;
+    }
+
+    nvmgpu_va_range = va_range->nvmgpu_va_range;
+
+    if (nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_VOLATILE)
+        uvm_mutex_lock(&nvmgpu_va_space->lock);
+
+    // Volatile data is simply discarded even though it has been remapped with non-volatile
+    for_each_va_block_in_va_range_safe(va_range, va_block, va_block_next) {
+        uvm_nvmgpu_block_clear_file_dirty(va_block);
+        if (nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_VOLATILE) {
+            uvm_nvmgpu_release_block(va_block);
+            list_del(&va_block->nvmgpu.lru);
+        }
+    }
+
+    if (nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_VOLATILE)
+        uvm_mutex_unlock(&nvmgpu_va_space->lock);
+
+    nvmgpu_va_range->flags = params->flags;
+
+    return NV_OK;
+}
+
+/**
+ * Unregister the specified va_range.
+ * NVMGPU will stop tracking this `va_range` after this point.
+ *
+ * @param va_range: va_range to be untracked.
+ *
+ * @return: always NV_OK.
+ */
+NV_STATUS uvm_nvmgpu_unregister_va_range(uvm_va_range_t *va_range)
+{
+    struct file *filp;
+
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = va_range->nvmgpu_va_range;
+
+    if (nvmgpu_va_range) 
+    {
+        filp = nvmgpu_va_range->filp;
+
+        if (nvmgpu_va_range->is_file_dirty_bitmaps)
+            uvm_kvfree(nvmgpu_va_range->is_file_dirty_bitmaps);
+
+        if (nvmgpu_va_range->has_data_bitmaps)
+            uvm_kvfree(nvmgpu_va_range->has_data_bitmaps);
+
+        if (nvmgpu_va_range->iov)
+            uvm_kvfree(nvmgpu_va_range->iov);
+
+        if (filp)
+        {
+            if ((nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_WRITE) && !(nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_VOLATILE))
+                vfs_fsync(filp, 1);
+
+            fput(filp);
+        }
+
+        uvm_kvfree(nvmgpu_va_range);
+    }
+
+    va_range->nvmgpu_va_range = NULL;
+
+    return NV_OK;
+}
+
+static void uvm_nvmgpu_unmap_page(uvm_va_block_t *va_block, int page_index)
+{
+    uvm_gpu_id_t id;
+
+    for_each_gpu_id(id) {
+        uvm_gpu_t *gpu;
+        uvm_va_block_gpu_state_t *gpu_state = va_block->gpus[uvm_id_gpu_index(id)];
+        if (!gpu_state)
+            continue;
+
+        if (gpu_state->cpu_pages_dma_addrs[page_index] == 0)
+            continue;
+
+        UVM_ASSERT(va_block->va_range);
+        UVM_ASSERT(va_block->va_range->va_space);
+        gpu = uvm_va_space_get_gpu(va_block->va_range->va_space, id);
+
+        uvm_gpu_unmap_cpu_page(gpu, gpu_state->cpu_pages_dma_addrs[page_index]);
+        gpu_state->cpu_pages_dma_addrs[page_index] = 0;
+    }
+}
+
+static NV_STATUS insert_pagecache_to_va_block(uvm_va_block_t *va_block, int page_id, struct page *page)
+{
+    NV_STATUS status = NV_OK;
+    uvm_gpu_id_t gpu_id;
+
+    lock_page(page);
+
+    if (va_block->cpu.pages[page_id] != page) {
+        if (va_block->cpu.pages[page_id] != NULL)
+            uvm_nvmgpu_unmap_page(va_block, page_id);
+
+        for_each_gpu_id(gpu_id) {
+            uvm_gpu_t *gpu;
+            uvm_va_block_gpu_state_t *gpu_state = va_block->gpus[uvm_id_gpu_index(gpu_id)];
+            if (!gpu_state)
+                continue;
+
+            UVM_ASSERT(gpu_state->cpu_pages_dma_addrs[page_id] == 0);
+
+            UVM_ASSERT(va_block->va_range);
+            UVM_ASSERT(va_block->va_range->va_space);
+            gpu = uvm_va_space_get_gpu(va_block->va_range->va_space, gpu_id);
+
+            status = uvm_gpu_map_cpu_pages(gpu, page, PAGE_SIZE, &gpu_state->cpu_pages_dma_addrs[page_id]);
+            if (status != NV_OK) {
+                printk(KERN_DEBUG "Cannot do uvm_gpu_map_cpu_pages\n");
+                goto insert_pagecache_to_va_block_error;
+            }
+        }
+        va_block->cpu.pages[page_id] = page;
+    }
+
+    return NV_OK;
+
+insert_pagecache_to_va_block_error:
+    uvm_nvmgpu_unmap_page(va_block, page_id);
+    unlock_page(page);
+
+    return status;
+}
+
+/**
+ * Inspired by generic_file_buffered_read in /mm/filemap.c.
+ */
+static int prepare_page_for_read(struct file *filp, loff_t ppos, uvm_va_block_t *va_block, int page_id)
+{
+    struct address_space *mapping = filp->f_mapping;
+    struct inode *inode = mapping->host;
+    struct file_ra_state *ra = &filp->f_ra;
+    pgoff_t index;
+    pgoff_t last_index;
+    pgoff_t prev_index;
+    unsigned long offset;      /* offset into pagecache page */
+    unsigned int prev_offset;
+    int error = 0;
+
+    index = ppos >> PAGE_SHIFT;
+    prev_index = ra->prev_pos >> PAGE_SHIFT;
+    prev_offset = ra->prev_pos & (PAGE_SIZE-1);
+    last_index = (ppos + PAGE_SIZE + PAGE_SIZE-1) >> PAGE_SHIFT;
+    offset = ppos & ~PAGE_MASK;
+
+    for (;;) {
+        struct page *page;
+        pgoff_t end_index;
+        loff_t isize;
+        unsigned long nr;
+        NV_STATUS ret;
+
+        cond_resched();
+find_page:
+        if (fatal_signal_pending(current)) {
+            error = -EINTR;
+            goto out;
+        }
+
+        page = find_get_page(mapping, index);
+        if (!page) {
+            page_cache_sync_readahead(mapping,
+                    ra, filp,
+                    index, last_index - index);
+            page = find_get_page(mapping, index);
+            if (unlikely(page == NULL))
+                goto no_cached_page;
+        }
+        if (PageReadahead(page)) {
+            page_cache_async_readahead(mapping,
+                    ra, filp, page,
+                    index, last_index - index);
+        }
+        if (!PageUptodate(page)) {
+            /*
+             * See comment in do_read_cache_page on why
+             * wait_on_page_locked is used to avoid unnecessarily
+             * serialisations and why it's safe.
+             */
+            error = wait_on_page_locked_killable(page);
+            if (unlikely(error))
+                goto readpage_error;
+            if (PageUptodate(page))
+                goto page_ok;
+
+            if (inode->i_blkbits == PAGE_SHIFT ||
+                    !mapping->a_ops->is_partially_uptodate)
+                goto page_not_up_to_date;
+            if (!trylock_page(page))
+                goto page_not_up_to_date;
+            /* Did it get truncated before we got the lock? */
+            if (!page->mapping)
+                goto page_not_up_to_date_locked;
+            if (!mapping->a_ops->is_partially_uptodate(page,
+                        offset, PAGE_SIZE))
+                goto page_not_up_to_date_locked;
+            unlock_page(page);
+        }
+page_ok:
+        /*
+         * i_size must be checked after we know the page is Uptodate.
+         *
+         * Checking i_size after the check allows us to calculate
+         * the correct value for "nr", which means the zero-filled
+         * part of the page is not copied back to userspace (unless
+         * another truncate extends the file - this is desired though).
+         */
+
+        isize = i_size_read(inode);
+        end_index = (isize - 1) >> PAGE_SHIFT;
+        if (unlikely(!isize || index > end_index)) {
+            put_page(page);
+            goto out;
+        }
+
+        /* nr is the maximum number of bytes to copy from this page */
+        nr = PAGE_SIZE;
+        if (index == end_index) {
+            nr = ((isize - 1) & ~PAGE_MASK) + 1;
+            if (nr <= offset) {
+                put_page(page);
+                goto out;
+            }
+        }
+        nr = nr - offset;
+
+        /* If users can be writing to this page using arbitrary
+         * virtual addresses, take care about potential aliasing
+         * before reading the page on the kernel side.
+         */
+        if (mapping_writably_mapped(mapping))
+            flush_dcache_page(page);
+
+        /*
+         * When a sequential read accesses a page several times,
+         * only mark it as accessed the first time.
+         */
+        if (prev_index != index || offset != prev_offset)
+            mark_page_accessed(page);
+        prev_index = index;
+
+        /*
+         * Ok, we have the page, and it's up-to-date, so
+         * now we can insert it to the va_block...
+         */
+        ret = insert_pagecache_to_va_block(va_block, page_id, page);
+        if (ret != NV_OK) {
+            error = ret;
+            goto out;
+        }
+
+        offset += PAGE_SIZE;
+        index += offset >> PAGE_SHIFT;
+        offset &= ~PAGE_MASK;
+        prev_offset = offset;
+
+        goto out;
+
+page_not_up_to_date:
+        /* Get exclusive access to the page ... */
+        error = lock_page_killable(page);
+        if (unlikely(error))
+            goto readpage_error;
+
+page_not_up_to_date_locked:
+        /* Did it get truncated before we got the lock? */
+        if (!page->mapping) {
+            unlock_page(page);
+            put_page(page);
+            continue;
+        }
+
+        /* Did somebody else fill it already? */
+        if (PageUptodate(page)) {
+            unlock_page(page);
+            goto page_ok;
+        }
+
+readpage:
+        /*
+         * A previous I/O error may have been due to temporary
+         * failures, eg. multipath errors.
+         * PG_error will be set again if readpage fails.
+         */
+        ClearPageError(page);
+        /* Start the actual read. The read will unlock the page. */
+        error = mapping->a_ops->readpage(filp, page);
+
+        if (unlikely(error)) {
+            if (error == AOP_TRUNCATED_PAGE) {
+                put_page(page);
+                error = 0;
+                goto find_page;
+            }
+            goto readpage_error;
+        }
+
+        if (!PageUptodate(page)) {
+            error = lock_page_killable(page);
+            if (unlikely(error))
+                goto readpage_error;
+            if (!PageUptodate(page)) {
+                if (page->mapping == NULL) {
+                    /*
+                     * invalidate_mapping_pages got it
+                     */
+                    unlock_page(page);
+                    put_page(page);
+                    goto find_page;
+                }
+                unlock_page(page);
+                ra->ra_pages /= 4;
+                error = -EIO;
+                goto readpage_error;
+            }
+            unlock_page(page);
+        }
+
+        goto page_ok;
+
+readpage_error:
+        /* UHHUH! A synchronous read error occurred. Report it */
+        put_page(page);
+        goto out;
+
+no_cached_page:
+        /*
+         * Ok, it wasn't cached, so we need to create a new
+         * page..
+         */
+        page = page_cache_alloc(mapping);
+        if (!page) {
+            error = -ENOMEM;
+            goto out;
+        }
+        error = add_to_page_cache_lru(page, mapping, index,
+                mapping_gfp_constraint(mapping, GFP_KERNEL));
+        if (error) {
+            put_page(page);
+            if (error == -EEXIST) {
+                error = 0;
+                goto find_page;
+            }
+            goto out;
+        }
+        goto readpage;
+    }
+
+    error = -EAGAIN;
+out:
+    ra->prev_pos = prev_index;
+    ra->prev_pos <<= PAGE_SHIFT;
+    ra->prev_pos |= prev_offset;
+
+    file_accessed(filp);
+    return error;
+}
+
+/**
+ * Prepare page-cache pages to be read.
+ *
+ * @param va_block: data will be put in this va_block.
+ *
+ * @param block_retry: need this to allocate memory pages and register them to
+ * this UVM range.
+ *
+ * @param service_context: need it the same as block_retry.
+ *
+ * @return: NV_OK on success. NV_ERR_* otherwise.
+ */
+NV_STATUS uvm_nvmgpu_read_begin(uvm_va_block_t *va_block, uvm_va_block_retry_t *block_retry, uvm_service_block_context_t *service_context)
+{
+    NV_STATUS status = NV_OK;
+
+    uvm_va_range_t *va_range = va_block->va_range;
+
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = va_range->nvmgpu_va_range;
+
+    #if UVM_NVMGPU_PROFILING
+    uvm_nvmgpu_va_space_t *nvmgpu_va_space = va_range->va_space->nvmgpu_va_space;
+
+    struct timespec tv_start, tv_end;
+    #endif
+
+    struct file *nvmgpu_file = nvmgpu_va_range->filp;
+
+    // Calculate the file offset based on the block start address.
+    loff_t file_start_offset = va_block->start - va_block->va_range->node.start;
+    loff_t offset;
+
+    int page_id; 
+
+    // Specify that the entire block is the region of concern.
+    uvm_va_block_region_t region = uvm_va_block_region_from_block(va_block);
+
+    uvm_page_mask_t read_mask;
+
+    // Record the original page mask and set the mask to all 1s.
+    uvm_page_mask_t original_page_mask;
+    uvm_page_mask_copy(&original_page_mask, &service_context->block_context.make_resident.page_mask);
+
+    uvm_page_mask_fill(&service_context->block_context.make_resident.page_mask);
+
+    UVM_ASSERT(nvmgpu_file != NULL);
+
+    if (!uvm_nvmgpu_block_has_data(va_block)) {
+        bool is_file_dirty = uvm_nvmgpu_block_file_dirty(va_block);
+
+        // Prevent block_populate_pages from allocating new pages
+        uvm_nvmgpu_block_set_file_dirty(va_block);
+
+        // Change this va_block's state: all pages are the resident of CPU.
+        status = uvm_va_block_make_resident(va_block,
+                                            block_retry,
+                                            &service_context->block_context,
+                                            UVM_ID_CPU,
+                                            region,
+                                            NULL,
+                                            NULL,
+                                            UVM_MAKE_RESIDENT_CAUSE_NVMGPU);
+
+        // Return the dirty state to the original
+        if (!is_file_dirty)
+            uvm_nvmgpu_block_clear_file_dirty(va_block);
+
+        if (status != NV_OK) {
+            printk(KERN_DEBUG "Cannot make temporary resident on CPU\n");
+            goto read_begin_err_0;
+        }
+
+        status = uvm_tracker_wait(&va_block->tracker);
+        if (status != NV_OK) {
+            printk(KERN_DEBUG "Cannot make temporary resident on CPU\n");
+            goto read_begin_err_0;
+        }
+    }
+
+    uvm_page_mask_fill(&read_mask);
+
+    #if UVM_NVMGPU_PROFILING
+    getnstimeofday(&tv_start);
+    #endif
+
+    // Fill in page-cache pages to va_block
+    for_each_va_block_page_in_region_mask(page_id, &read_mask, region) {
+        offset = file_start_offset + page_id * PAGE_SIZE;
+
+        if (prepare_page_for_read(nvmgpu_file, offset, va_block, page_id) != 0) {
+            printk(KERN_DEBUG "Cannot prepare page for read at file offset 0x%llx\n", offset);
+            status = NV_ERR_OPERATING_SYSTEM;
+            goto read_begin_err_0;
+        }
+
+        UVM_ASSERT(va_block->cpu.pages[page_id]);
+    }
+
+    #if UVM_NVMGPU_PROFILING
+    getnstimeofday(&tv_end);
+
+    nvmgpu_va_space->time.pc_read_file += uvm_nvmgpu_timediff(tv_start, tv_end);
+    #endif
+
+    uvm_nvmgpu_block_set_has_data(va_block);
+
+read_begin_err_0:
+    // Put back the original mask.
+    uvm_page_mask_copy(&service_context->block_context.make_resident.page_mask, &original_page_mask);
+    
+    return status;
+}
+
+NV_STATUS uvm_nvmgpu_read_end(uvm_va_block_t *va_block)
+{
+    int page_id;
+    struct page *page;
+
+    uvm_page_mask_t read_mask;
+
+    uvm_page_mask_fill(&read_mask);
+    for_each_va_block_page_in_mask(page_id, &read_mask, va_block) {
+        page = va_block->cpu.pages[page_id];
+        if (page) {
+            unlock_page(page);
+            put_page(page);
+        }
+    }
+
+    return NV_OK;
+}
+
+/**
+ * Get a free (not-currently-used) read buffer and lock it.
+ */
+static inline uvm_nvmgpu_gdirect_buffer_t *nvmgpu_get_free_read_buffer_with_lock(uvm_nvmgpu_gdirect_t *gdirect)
+{
+    uvm_nvmgpu_gdirect_buffer_t *buffer;
+    unsigned int i;
+
+    if (gdirect->num_read_buffers == 1)
+    {
+        buffer = gdirect->read_buffers;
+        uvm_mutex_lock(&buffer->lock);
+        return buffer;
+    }
+
+    while (1)
+    {
+        for (i = 0; i < gdirect->num_read_buffers; ++i)
+        {
+            buffer = &gdirect->read_buffers[i];
+            if (uvm_mutex_trylock(&buffer->lock))
+                return buffer;
+        }
+        // Yield to another process since all buffers are being used now.
+        cond_resched();
+    }
+}
+
+static NV_STATUS nvmgpu_direct_io_copy_range(
+    uvm_va_block_t *va_block, uvm_nvmgpu_gdirect_buffer_t *buffer,
+    uvm_page_index_t page_idx_start, int num_pages,
+    uvm_push_t *push, bool is_last_range
+)
+{
+    uvm_va_range_t *va_range = va_block->va_range;
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = va_range->nvmgpu_va_range;
+    uvm_nvmgpu_gdirect_t *gdirect = buffer->gdirect;
+    uvm_gpu_t *gpu = gdirect->gpu;
+
+    uvm_va_block_gpu_state_t *gpu_state = va_block->gpus[uvm_id_gpu_index(gpu->id)];
+
+    struct file *filp = nvmgpu_va_range->filp;
+    size_t file_size = i_size_read(filp->f_mapping->host);
+
+    // Calculate the file offset and read size.
+    loff_t va_block_f_offset = va_block->start - va_range->node.start;
+    loff_t f_offset = va_block_f_offset + page_idx_start * PAGE_SIZE;
+    size_t copied_size = 0;
+    size_t read_size; 
+    
+    if (likely(f_offset < file_size))
+        read_size = MIN(num_pages * PAGE_SIZE, file_size - f_offset);
+    else
+        read_size = 0;
+
+    while (copied_size < read_size)
+    {
+        size_t to_copy_size;
+        loff_t chunk_offset;
+
+        uvm_chunk_size_t chunk_size;
+        size_t chunk_idx;
+        uvm_gpu_chunk_t *chunk;
+
+        uvm_gpu_address_t dst;
+        uvm_gpu_address_t src;
+
+        uvm_page_index_t page_index = page_idx_start + copied_size / PAGE_SIZE;
+
+        chunk_idx = uvm_va_block_gpu_chunk_index_range(va_block->start, uvm_va_block_size(va_block), gpu, page_index, &chunk_size);
+        chunk = gpu_state->chunks[chunk_idx];
+
+        UVM_ASSERT(chunk);
+        UVM_ASSERT(chunk_size >= PAGE_SIZE);
+        UVM_ASSERT(chunk->va_block_page_index <= page_index);
+
+        chunk_offset = (page_index - chunk->va_block_page_index) * PAGE_SIZE;
+        to_copy_size = MIN(read_size - copied_size, chunk_size - chunk_offset);
+
+        UVM_ASSERT(to_copy_size > 0);
+
+        dst = uvm_gpu_address_physical(UVM_APERTURE_VID, chunk->address + chunk_offset);
+        src = uvm_gpu_address_virtual(buffer->aligned_rm_gpu_va + page_index * PAGE_SIZE);
+
+        // Pipeline all copies.
+        uvm_push_set_flag(push, UVM_PUSH_FLAG_CE_NEXT_PIPELINED);
+
+        if (is_last_range && copied_size + to_copy_size == read_size)
+            // Last copy so include membar.gl.
+            uvm_push_set_flag(push, UVM_PUSH_FLAG_CE_NEXT_MEMBAR_GPU);
+        else
+            // In between copies, no need for membar.
+            uvm_push_set_flag(push, UVM_PUSH_FLAG_CE_NEXT_MEMBAR_NONE);
+
+        gpu->ce_hal->memcopy(push, dst, src, to_copy_size);
+
+        copied_size += to_copy_size;
+    }
+
+    return NV_OK;
+}
+
+/**
+ * Perform read using GPUDirect. This function currently works with NVMe PCI storage devices only. 
+ * How it works:
+ * 1. Transfer data from file to RM mem on GPU.
+ * 2. Copy from RM mem to the corresponding GPU chunks.
+ *
+ * @param va_block: va_block to work with
+ * @param block_retry: use in uvm_va_block_make_resident
+ * @param service_context: use in uvm_va_block_make_resident
+ * @param cause: use in uvm_va_block_make_resident
+ * @param gpu_id: Destination GPU ID
+ * @param page_mask: Hint for pages to be transferred. May not be respected.
+ *
+ * @return: NV_OK on success. NV_ERR_* otherwise.
+ */
+NV_STATUS uvm_nvmgpu_direct_io_read(
+    uvm_va_block_t *va_block, 
+    uvm_va_block_retry_t *block_retry, 
+    uvm_service_block_context_t *service_context,
+    uvm_make_resident_cause_t cause,
+    uvm_gpu_id_t gpu_id, 
+    const uvm_page_mask_t *page_mask
+)
+{
+    NV_STATUS status = NV_OK;
+
+    uvm_va_range_t *va_range = va_block->va_range;
+    uvm_va_space_t *va_space = va_range->va_space;
+
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = va_range->nvmgpu_va_range;
+
+    struct file *filp = nvmgpu_va_range->filp;
+    size_t file_size = i_size_read(filp->f_mapping->host);
+    loff_t f_offset = va_block->start - va_range->node.start;
+    struct iov_iter iter;
+    size_t read_size;
+    struct kiocb kiocb;
+    ssize_t retcode;
+
+    mm_segment_t fs;
+
+    uvm_gpu_t *gpu = uvm_va_space_get_gpu(va_space, gpu_id);
+    uvm_push_t push;
+    bool did_push_begin = false;
+
+    uvm_page_index_t page_idx;
+    int page_idx_start = -1;
+    int num_pages = 0;
+
+    uvm_page_mask_t copy_page_mask;
+    uvm_processor_mask_t src_processor_mask;
+    uvm_gpu_id_t src_gpu_id;
+
+    uvm_nvmgpu_va_space_t *nvmgpu_va_space = va_space->nvmgpu_va_space;
+    uvm_nvmgpu_gdirect_t *gdirect = &nvmgpu_va_space->gdirect_table[uvm_id_gpu_index(gpu_id)];
+    uvm_nvmgpu_gdirect_buffer_t *buffer;
+
+    #if UVM_NVMGPU_PROFILING
+    struct timespec tv_start, tv_end;
+    #endif
+
+    // We will copy data from other residents. The leftover will be from the file.
+    if (!uvm_processor_mask_test(&va_block->resident, gpu_id))
+        uvm_page_mask_fill(&copy_page_mask);
+    else
+        uvm_page_mask_andnot(&copy_page_mask, page_mask, &va_block->gpus[uvm_id_gpu_index(gpu_id)]->resident);
+
+    uvm_processor_mask_and(&src_processor_mask, &va_space->can_copy_from[uvm_id_value(gpu_id)], &va_block->resident);
+    uvm_processor_mask_clear(&src_processor_mask, gpu_id);
+
+    if (uvm_processor_mask_test_and_clear(&src_processor_mask, UVM_ID_CPU))
+        uvm_page_mask_andnot(&copy_page_mask, &copy_page_mask, &va_block->cpu.resident);
+
+    for_each_gpu_id_in_mask(src_gpu_id, &src_processor_mask)
+        uvm_page_mask_andnot(&copy_page_mask, &copy_page_mask, &va_block->gpus[uvm_id_gpu_index(src_gpu_id)]->resident);
+
+    status = uvm_va_block_make_resident(va_block,
+                                        block_retry,
+                                        &service_context->block_context,
+                                        gpu_id,
+                                        service_context->region,
+                                        &copy_page_mask,
+                                        NULL,
+                                        cause);
+    
+    if (status != NV_OK)
+    {
+        if (unlikely(status != NV_ERR_MORE_PROCESSING_REQUIRED))
+            printk(KERN_DEBUG "uvm_va_block_make_resident error %s\n", nvstatusToString(status));
+        return status;
+    }
+
+    // No need to copy from the file. So exit.
+    if (unlikely(uvm_page_mask_empty(&copy_page_mask)))
+        return NV_OK;
+
+    // Not in the file's range. Nothing to copy.
+    if (unlikely(f_offset >= file_size))
+        return NV_OK;
+
+    #if UVM_NVMGPU_PROFILING
+    getnstimeofday(&tv_start);
+    #endif
+
+    buffer = nvmgpu_get_free_read_buffer_with_lock(gdirect);
+
+    read_size = MIN(MIN(buffer->num_pages * PAGE_SIZE, file_size - f_offset), uvm_va_block_size(va_block));
+
+    // Switch the filesystem space to kernel space.
+    fs = get_fs();
+    set_fs(KERNEL_DS);
+
+    // Prepare descriptor for read.
+    iov_iter_bvec(&iter, READ, buffer->bvecs, buffer->num_pages, UVM_ALIGN_UP(read_size, PAGE_SIZE));
+    init_sync_kiocb(&kiocb, filp);
+    kiocb.ki_pos = f_offset;
+
+    retcode = call_read_iter(filp, &kiocb, &iter);
+
+    // Return to the original filesystem space.
+    set_fs(fs);
+
+    if (unlikely(retcode != read_size))
+    {
+        printk("error %ld in call_read_iter\n", retcode);
+        status = NV_ERR_OPERATING_SYSTEM;
+        goto error;
+    }
+
+    #if UVM_NVMGPU_PROFILING
+    getnstimeofday(&tv_end);
+    nvmgpu_va_space->time.dio_read_file += uvm_nvmgpu_timediff(tv_start, tv_end);
+
+    getnstimeofday(&tv_start);
+    #endif
+
+    // Initiate a GPU command push channel.
+    status = uvm_push_begin_acquire(
+        gpu->channel_manager, 
+        UVM_CHANNEL_TYPE_GPU_INTERNAL, 
+        &va_block->tracker,
+        &push, 
+        "nvmgpu copy rm buf to chunk(s)"
+    );
+    if (status != NV_OK)
+    {
+        printk(KERN_DEBUG "uvm_push_begin_acquire error %s\n", nvstatusToString(status));
+        goto error;
+    }
+    did_push_begin = true;
+
+    for_each_va_block_page_in_mask(page_idx, &copy_page_mask, va_block)
+    {
+        if (page_idx_start < 0)
+        {
+            // First page.
+            page_idx_start = page_idx;
+            num_pages = 1;
+        }
+        else if (page_idx == page_idx_start + num_pages)
+            // Contiguous pages. Keep moving.
+            ++num_pages;
+        else
+        {
+            // Found a non-contiguous page.
+            // Initiate copy for the previous contiguous pages.
+            status = nvmgpu_direct_io_copy_range(va_block, buffer, page_idx_start, num_pages, &push, false);
+            if (status != NV_OK)
+                goto error;
+
+            // Reset index
+            page_idx_start = page_idx;
+            num_pages = 1;
+        }
+    }
+
+    // Do read for the last group of contiguous pages.
+    if (num_pages > 0)
+        status = nvmgpu_direct_io_copy_range(va_block, buffer, page_idx_start, num_pages, &push, true);
+
+error:
+    if (did_push_begin)
+        status = uvm_push_end_and_wait(&push);
+    
+    uvm_mutex_unlock(&buffer->lock);
+
+    #if UVM_NVMGPU_PROFILING
+    if (status == NV_OK)
+    {
+        getnstimeofday(&tv_end);
+
+        nvmgpu_va_space->time.dio_read_memcopy += uvm_nvmgpu_timediff(tv_start, tv_end);
+    }
+    #endif
+
+    return status;
+}
+
+static NV_STATUS nvmgpu_direct_io_write_range(
+    uvm_va_block_t *va_block, uvm_nvmgpu_gdirect_buffer_t *buffer, 
+    uvm_page_index_t page_index_start, int num_pages,
+    size_t *written_bytes
+)
+{
+    uvm_va_range_t *va_range = va_block->va_range;
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = va_range->nvmgpu_va_range;
+
+    struct file *filp = nvmgpu_va_range->filp;
+    struct kiocb kiocb;
+    struct iov_iter iter;
+    ssize_t retcode;
+
+    // Calculate the file offset and write size.
+    loff_t file_start_offset = va_block->start - va_range->node.start + page_index_start * PAGE_SIZE;
+    size_t write_size = MIN(MIN(num_pages * PAGE_SIZE, uvm_va_range_size(va_range) - file_start_offset), uvm_va_block_size(va_block));
+
+    // Prepare write descriptor.
+    iov_iter_bvec(&iter, WRITE, &buffer->bvecs[page_index_start], num_pages, UVM_ALIGN_UP(write_size, PAGE_SIZE));
+
+    init_sync_kiocb(&kiocb, filp);
+    kiocb.ki_pos = file_start_offset;
+
+    // Write from RM memory to the file.
+    // It uses direct_io in the background because the file is opened with O_DIRECT.
+    retcode = call_write_iter(filp, &kiocb, &iter);
+    if (retcode != write_size)
+    {
+        printk(KERN_DEBUG "error in call_write_iter with code %ld\n", retcode);
+        return NV_ERR_OPERATING_SYSTEM;
+    }
+
+    *written_bytes = retcode;
+
+    return NV_OK;
+}
+
+static NV_STATUS nvmgpu_direct_io_write_from_gpu(uvm_va_block_t *va_block, uvm_gpu_id_t gpu_id, const uvm_page_mask_t *page_mask)
+{
+    NV_STATUS status = NV_OK;
+
+    uvm_va_range_t *va_range = va_block->va_range;
+    uvm_va_space_t *va_space = va_range->va_space;
+
+    uvm_nvmgpu_va_space_t *nvmgpu_va_space = va_space->nvmgpu_va_space;
+
+    uvm_nvmgpu_gdirect_t *gdirect = &nvmgpu_va_space->gdirect_table[uvm_id_gpu_index(gpu_id)];
+    uvm_nvmgpu_gdirect_buffer_t *buffer = gdirect->write_buffer;
+
+    uvm_gpu_t *gpu = uvm_va_space_get_gpu(va_space, gpu_id);
+    uvm_va_block_gpu_state_t *gpu_state = va_block->gpus[uvm_id_gpu_index(gpu_id)];
+    uvm_push_t push;
+
+    uvm_page_index_t page_index;
+    int page_index_start = -1;
+    int num_pages = 0;
+    uvm_va_block_region_t region = uvm_va_block_region_from_block(va_block);
+    
+    mm_segment_t fs;
+
+    struct file *filp = va_range->nvmgpu_va_range->filp;
+    size_t file_written_bytes = 0;
+    size_t written_bytes = 0;
+
+    bool writing_started = false;
+
+    // Nothing in the page_mask. 
+    if (uvm_page_mask_weight(page_mask) == 0)
+        return NV_OK;
+
+    uvm_mutex_lock(&buffer->lock);
+
+    // Initiate a GPU command push channel.
+    status = uvm_push_begin_acquire(
+        gpu->channel_manager, 
+        UVM_CHANNEL_TYPE_GPU_INTERNAL, 
+        &va_block->tracker,
+        &push, 
+        "nvmgpu copy chunk(s) to rm buf"
+    );
+    if (status != NV_OK)
+    {
+        printk(KERN_DEBUG "uvm_push_begin_acquire error %s\n", nvstatusToString(status));
+        goto error;
+    }
+
+    for_each_va_block_page_in_region_mask(page_index, page_mask, region)
+    {
+        uvm_gpu_address_t dst;
+        uvm_gpu_address_t src;
+
+        size_t to_copy_size;
+
+        loff_t chunk_offset;
+        uvm_chunk_size_t chunk_size;
+        size_t chunk_idx;
+        uvm_gpu_chunk_t *chunk;
+
+        uvm_page_index_t next_page_index;
+
+        chunk_idx = uvm_va_block_gpu_chunk_index_range(va_block->start, uvm_va_block_size(va_block), gpu, page_index, &chunk_size);
+        chunk = gpu_state->chunks[chunk_idx];
+
+        UVM_ASSERT(chunk);
+        UVM_ASSERT(chunk_size >= PAGE_SIZE);
+        UVM_ASSERT(chunk->va_block_page_index <= page_index);
+
+        chunk_offset = (page_index - chunk->va_block_page_index) * PAGE_SIZE;
+        to_copy_size = chunk_size - chunk_offset;
+
+        next_page_index = uvm_va_block_next_page_in_mask(region, page_mask, page_index + to_copy_size / PAGE_SIZE - 1);
+
+        // Copy the whole chunk.
+        src = uvm_gpu_address_physical(UVM_APERTURE_VID, chunk->address + chunk_offset);
+        dst = uvm_gpu_address_virtual(buffer->aligned_rm_gpu_va + page_index * PAGE_SIZE);
+
+        // Pipeline all copies.
+        uvm_push_set_flag(&push, UVM_PUSH_FLAG_CE_NEXT_PIPELINED);
+
+        if (next_page_index == region.outer)
+            // Last copy so include membar.gl.
+            uvm_push_set_flag(&push, UVM_PUSH_FLAG_CE_NEXT_MEMBAR_GPU);
+        else
+            // In between copies, no need for membar.
+            uvm_push_set_flag(&push, UVM_PUSH_FLAG_CE_NEXT_MEMBAR_NONE);
+
+        gpu->ce_hal->memcopy(&push, dst, src, to_copy_size);
+
+        // Skip all pages covered by this chunk.
+        page_index = next_page_index - 1;
+    }
+
+    // Wait for all copies to finish.
+    status = uvm_push_end_and_wait(&push);
+    if (status != NV_OK)
+    {
+        printk(KERN_DEBUG "uvm_push_end_and_wait error %s\n", nvstatusToString(status));
+        goto error;
+    }
+
+    // Switch the filesystem space to kernel space.
+    fs = get_fs();
+    set_fs(KERNEL_DS);
+
+    file_start_write(filp);
+
+    writing_started = true;
+
+    for_each_va_block_page_in_mask(page_index, page_mask, va_block)
+    {
+        if (page_index_start < 0)
+        {
+            // First page.
+            page_index_start = page_index;
+            num_pages = 1;
+        }
+        else if (page_index == page_index_start + num_pages)
+            // Contiguous pages. Keep moving.
+            ++num_pages;
+        else
+        {
+            // Found a non-contiguous page.
+            // Initiate write for the previous contiguous pages.
+            status = nvmgpu_direct_io_write_range(va_block, buffer, page_index_start, num_pages, &written_bytes);
+            if (status != NV_OK)
+                goto error;
+
+            file_written_bytes += written_bytes;
+
+            // Reset index
+            page_index_start = page_index;
+            num_pages = 1;
+        }
+    }
+
+    // Do write for the last group of contiguous pages.
+    if (num_pages > 0)
+    {
+        status = nvmgpu_direct_io_write_range(va_block, buffer, page_index_start, num_pages, &written_bytes);
+        if (status != NV_OK)
+            goto error;
+
+        file_written_bytes += written_bytes;
+    }
+
+error:
+    if (writing_started)
+    {
+        if (file_written_bytes > 0)
+        {
+            fsnotify_modify(filp);
+            add_wchar(current, file_written_bytes);
+        }
+        inc_syscw(current);
+
+        file_end_write(filp);
+
+        // Return to the original filesystem space.
+        set_fs(fs);
+    }
+    uvm_mutex_unlock(&buffer->lock);
+
+    return status;
+}
+
+/**
+ * Copy data from GPUs to the backing file using GPUDirect.
+ * How it works:
+ * 1. Iterate through all GPUs that have data associated with this va_block in the page_mask:
+ *    1.1. Transfer the data on the GPU to the RM mem on that GPU.
+ *    1.2. Copy from the RM mem to the backing file.
+
+ * @param va_block: va_block to work with
+ * @param page_mask: Transfer only the specified pages in this va_block. This is destructive.
+ *
+ * @return: NV_OK on success. NV_ERR_* otherwise.
+ */
+NV_STATUS uvm_nvmgpu_direct_io_write(uvm_va_block_t *va_block, uvm_page_mask_t *page_mask)
+{
+    NV_STATUS status = NV_OK;
+
+    uvm_processor_id_t src_id;
+    uvm_processor_mask_t search_mask;
+
+    // Nothing to copy.
+    if (uvm_page_mask_weight(page_mask) == 0)
+        return NV_OK;
+
+    // All GPUs associated with this va_block.
+    uvm_processor_mask_copy(&search_mask, &va_block->resident);
+    uvm_processor_mask_clear(&search_mask, UVM_ID_CPU);
+
+    for_each_closest_id(src_id, &search_mask, UVM_ID_CPU, va_block->va_range->va_space) 
+    {
+        uvm_page_mask_t *src_resident_mask = uvm_va_block_resident_mask_get(va_block, src_id);
+        uvm_page_mask_t src_copy_page_mask;
+
+        uvm_page_mask_and(&src_copy_page_mask, page_mask, src_resident_mask);
+
+        // No need to copy from this src.
+        if (uvm_page_mask_weight(&src_copy_page_mask) == 0)
+            continue;
+
+        // Do the actual copy
+        status = nvmgpu_direct_io_write_from_gpu(va_block, src_id, &src_copy_page_mask);
+        if (status != NV_OK)
+        {
+            printk(KERN_DEBUG "nvmgpu_direct_io_write_from_gpu error %s\n", nvstatusToString(status));
+            return status;
+        }
+
+        uvm_page_mask_andnot(page_mask, page_mask, &src_copy_page_mask);
+
+        // Nothing left to copy.
+        if (uvm_page_mask_weight(page_mask) == 0)
+            break;
+    }
+
+    return NV_OK;
+}
+
+/**
+ * Evict out the block. This function can handle both CPU-only and GPU blocks.
+ * 
+ * @param va_block: the block to be evicted.
+ * 
+ * @return: NV_OK on success. NV_ERR_* otherwise.
+ */
+NV_STATUS uvm_nvmgpu_flush_block(uvm_va_block_t *va_block)
+{
+    NV_STATUS status = NV_OK;
+    uvm_va_range_t *va_range = va_block->va_range;
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = va_range->nvmgpu_va_range;
+
+    if (!(nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_WRITE))
+        return NV_OK;
+
+    // Move data from GPU to CPU
+    if (uvm_processor_mask_get_gpu_count(&(va_block->resident)) > 0) {
+        uvm_va_block_region_t region = uvm_va_block_region_from_block(va_block);
+        uvm_va_block_context_t *block_context = uvm_va_block_context_alloc();
+
+        if (!block_context) {
+            printk(KERN_DEBUG "NV_ERR_NO_MEMORY\n");
+            return NV_ERR_NO_MEMORY;
+        }
+
+        if (nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_DIRECT)
+        {
+            // For DIRECT IO, we ask eviction from all GPUs.
+            status = uvm_va_block_make_resident(va_block,
+                                                NULL,
+                                                block_context,
+                                                UVM_ID_CPU,
+                                                region,
+                                                NULL,
+                                                NULL,
+                                                UVM_MAKE_RESIDENT_CAUSE_EVICTION);
+        }
+        else
+        {
+            // Force direct flush into the file for UVM_NVMGPU_FLAG_USEHOSTBUF that has no host buffer
+            if ((nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_USEHOSTBUF) 
+                && !va_block->nvmgpu.use_uvm_buffer
+            )
+                uvm_nvmgpu_block_set_file_dirty(va_block);
+
+            // Move data resided on the GPU to host.
+            // Data is automatically moved to the file if UVM_NVMGPU_FLAG_USEHOSTBUF is unset.
+            status = uvm_va_block_migrate_locked(
+                va_block, 
+                NULL, 
+                block_context, 
+                region, 
+                UVM_ID_CPU, 
+                UVM_MIGRATE_MODE_MAKE_RESIDENT, 
+                NULL
+            );
+        }
+
+        uvm_va_block_context_free(block_context);
+
+        if (status != NV_OK) {
+            printk(KERN_DEBUG "NOT NV_OK\n");
+            return status;
+        }
+
+        // Wait for the d2h transfer to complete.
+        status = uvm_tracker_wait(&va_block->tracker);
+
+        if (status != NV_OK) {
+            printk(KERN_DEBUG "NOT NV_OK\n");
+            return status;
+        }
+    }
+
+    // Flush the data kept in the host memory
+    if ((nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_USEHOSTBUF)
+        && va_block->nvmgpu.use_uvm_buffer
+    ) {
+        status = uvm_nvmgpu_flush_host_block(va_block, false, NULL);
+        if (status != NV_OK) {
+            printk(KERN_DEBUG "CANNOT FLUSH HOST BLOCK\n");
+            return status;
+        }
+    }
+
+    return status;
+}
+
+/**
+ * Flush all blocks in the `va_range`. 
+ *
+ * @param va_range: va_range that we want to flush the data.
+ *
+ * @return: NV_OK on success. NV_ERR_* otherwise.
+ */
+NV_STATUS uvm_nvmgpu_flush(uvm_va_range_t *va_range)
+{
+    NV_STATUS status = NV_OK;
+    uvm_va_block_t *va_block, *va_block_next;
+
+    // Evict blocks one by one.
+    for_each_va_block_in_va_range_safe(va_range, va_block, va_block_next) {
+        if ((status = uvm_nvmgpu_flush_block(va_block)) != NV_OK) {
+            printk(KERN_DEBUG "Encountered a problem with uvm_nvmgpu_flush_block\n");
+            break;
+        }
+    }
+
+    return status;
+}
+
+
+/**
+ * Free memory associated with the `va_block`.
+ *
+ * @param va_block: va_block to be freed.
+ * 
+ * @return: always NV_OK;
+ */
+NV_STATUS uvm_nvmgpu_release_block(uvm_va_block_t *va_block)
+{
+    uvm_va_block_t *old;
+    size_t index;
+    
+    uvm_va_range_t *va_range = va_block->va_range;
+
+    UVM_ASSERT(va_block != NULL);
+
+    // Remove the block from the list.
+    index = uvm_va_range_block_index(va_range, va_block->start);
+    old = (uvm_va_block_t *)nv_atomic_long_cmpxchg(&va_range->blocks[index],
+                                                  (long)va_block,
+                                                  (long)NULL);
+
+    // Free the block.
+    if (old == va_block) {
+        uvm_nvmgpu_block_clear_has_data(va_block);
+        uvm_va_block_kill(va_block);
+    }
+
+    return NV_OK;
+}
+
+NV_STATUS uvm_nvmgpu_prepare_block_for_hostbuf(uvm_va_block_t *va_block)
+{
+    int page_id;
+    if (!va_block->nvmgpu.use_uvm_buffer) {
+        for (page_id = 0; page_id < PAGES_PER_UVM_VA_BLOCK; ++page_id) {
+            if (va_block->cpu.pages[page_id] != NULL) {
+                uvm_nvmgpu_unmap_page(va_block, page_id);
+                va_block->cpu.pages[page_id] = NULL;
+            }
+        }
+    }
+    return NV_OK;
+}
+
+NV_STATUS uvm_nvmgpu_write_begin(uvm_va_block_t *va_block, bool is_flush)
+{
+    NV_STATUS status = NV_OK;
+
+    int page_id;
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = va_block->va_range->nvmgpu_va_range;
+
+    // Calculate the file offset based on the block start address.
+    loff_t file_start_offset = va_block->start - va_block->va_range->node.start;
+    loff_t file_position;
+
+    struct file *nvmgpu_file = nvmgpu_va_range->filp;
+    struct inode *f_inode = file_inode(nvmgpu_file);
+    struct address_space *mapping = nvmgpu_file->f_mapping;
+    struct inode *m_inode = mapping->host;
+    const struct address_space_operations *a_ops = mapping->a_ops;
+
+    struct page *page;
+    void *fsdata;
+
+    uvm_va_space_t *va_space;
+
+    UVM_ASSERT(va_block->va_range);
+    UVM_ASSERT(va_block->va_range->va_space);
+    va_space = va_block->va_range->va_space;
+
+    inode_lock(f_inode);
+
+    current->backing_dev_info = inode_to_bdi(m_inode);
+
+    file_remove_privs(nvmgpu_file);
+
+    file_update_time(nvmgpu_file);
+
+    for (page_id = 0; page_id < PAGES_PER_UVM_VA_BLOCK; ++page_id) {
+        uvm_gpu_id_t id;
+        long f_status = 0;
+
+        file_position = file_start_offset + page_id * PAGE_SIZE;
+
+        if (file_position >= nvmgpu_va_range->size)
+            break;
+
+        f_status = a_ops->write_begin(
+            nvmgpu_file, 
+            mapping, 
+            file_position, 
+            MIN(PAGE_SIZE, nvmgpu_va_range->size - file_position), 
+            0, 
+            &page, 
+            &fsdata
+        );
+        
+        if (f_status != 0 || page == NULL)
+            continue;
+
+        if (mapping_writably_mapped(mapping))
+            flush_dcache_page(page);
+
+        fsdata_array[page_id] = fsdata;
+
+        if (va_block->cpu.pages[page_id] != NULL)
+            uvm_nvmgpu_unmap_page(va_block, page_id);
+
+        for_each_gpu_id(id) {
+            uvm_gpu_t *gpu;
+            uvm_va_block_gpu_state_t *gpu_state = va_block->gpus[uvm_id_gpu_index(id)];
+            if (!gpu_state)
+                continue;
+
+            UVM_ASSERT(gpu_state->cpu_pages_dma_addrs[page_id] == 0);
+
+            gpu = uvm_va_space_get_gpu(va_space, id);
+
+            status = uvm_gpu_map_cpu_pages(gpu, page, PAGE_SIZE, &gpu_state->cpu_pages_dma_addrs[page_id]);
+            UVM_ASSERT(status == NV_OK);
+        }
+
+        va_block->cpu.pages[page_id] = page;
+    }
+
+    return status;
+}
+
+NV_STATUS uvm_nvmgpu_write_end(uvm_va_block_t *va_block, bool is_flush)
+{
+    NV_STATUS status = NV_OK;
+
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = va_block->va_range->nvmgpu_va_range;
+    struct file *nvmgpu_file = nvmgpu_va_range->filp;
+    struct inode *f_inode = file_inode(nvmgpu_file);
+    struct address_space *mapping = nvmgpu_file->f_mapping;
+    const struct address_space_operations *a_ops = mapping->a_ops;
+
+    int page_id;
+
+    loff_t file_start_offset = va_block->start - va_block->va_range->node.start;
+    loff_t file_position;
+
+    for (page_id = 0; page_id < PAGES_PER_UVM_VA_BLOCK; ++page_id) {
+        struct page *page = va_block->cpu.pages[page_id];
+        void *fsdata = fsdata_array[page_id];
+
+        file_position = file_start_offset + page_id * PAGE_SIZE;
+
+        if (file_position >= nvmgpu_va_range->size)
+            break;
+
+        if (page) {
+            size_t bytes = MIN(PAGE_SIZE, nvmgpu_va_range->size - file_position);
+            flush_dcache_page(page);
+            mark_page_accessed(page);
+
+            a_ops->write_end(
+                nvmgpu_file, 
+                mapping, 
+                file_position, 
+                bytes, 
+                bytes, 
+                page, 
+                fsdata
+            );
+
+            balance_dirty_pages_ratelimited(mapping);
+        }
+    }
+
+    uvm_nvmgpu_block_set_has_data(va_block);
+    uvm_nvmgpu_block_set_file_dirty(va_block);
+
+    current->backing_dev_info = NULL;
+
+    inode_unlock(f_inode);
+
+    return status;
+}
+
+/**
+ * Automatically reduce memory usage if we need to.
+ * 
+ * @param va_space: va_space that governs this operation.
+ * @param force: if true, we will evict some blocks without checking for the memory pressure.
+ *
+ * @return: NV_OK on success, NV_ERR_* otherwise.
+ */
+NV_STATUS uvm_nvmgpu_reduce_memory_consumption(uvm_va_space_t *va_space)
+{
+    NV_STATUS status = NV_OK;
+
+    uvm_nvmgpu_va_space_t *nvmgpu_va_space = va_space->nvmgpu_va_space;
+
+    unsigned long counter = 0;
+
+    uvm_va_block_t *va_block;
+
+    // Reclaim blocks based on least recent transfer.
+    uvm_mutex_lock(&nvmgpu_va_space->lock);
+    while (!list_empty(&nvmgpu_va_space->lru_head) && counter < nvmgpu_va_space->trash_nr_blocks) {
+        va_block = list_first_entry(&nvmgpu_va_space->lru_head, uvm_va_block_t, nvmgpu.lru);
+
+        // Terminate the loop since we cannot trash out blocks that have a copy on GPU
+        if (uvm_processor_mask_get_gpu_count(&(va_block->resident)) > 0) {
+            printk(KERN_DEBUG "Encounter a block whose data are in GPU!!!\n");
+            break;
+        }
+
+        // Evict the block if it is on CPU only and this `va_range` has the write flag.
+        if (uvm_processor_mask_get_count(&(va_block->resident)) > 0 && va_block->va_range->nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_WRITE) {
+            status = uvm_nvmgpu_flush_host_block(va_block, true, NULL);
+            if (status != NV_OK) {
+                printk(KERN_DEBUG "Cannot evict block\n");
+                break;
+            }
+        }
+
+        // Remove this block from the list and release it.
+        list_del(nvmgpu_va_space->lru_head.next);
+        uvm_nvmgpu_release_block(va_block);
+        ++counter;
+    }
+    uvm_mutex_unlock(&nvmgpu_va_space->lock);
+
+    return status;
+}
+
+/**
+ * Write the data of this `va_block` to the file.
+ * Callers have to make sure that there is no duplicated data on GPU.
+ * 
+ * @param va_block: the data source.
+ * @param is_evict: indicate that this function is called do to eviction not flush.
+ * @param page_mask: indicate which pages to be written out to the file. Ignore
+ * if NULL.
+ *
+ * @return: NV_OK on success. NV_ERR_* otherwise.
+ */
+NV_STATUS uvm_nvmgpu_flush_host_block(uvm_va_block_t *va_block, bool is_evict, const uvm_page_mask_t *page_mask)
+{
+    NV_STATUS status = NV_OK;
+
+    uvm_va_range_t *va_range = va_block->va_range;
+
+    struct file *nvmgpu_file = va_range->nvmgpu_va_range->filp;
+    mm_segment_t fs;
+
+    int page_id, prev_page_id;
+
+    // Compute the file start offset based on `va_block`.
+    loff_t file_start_offset = va_block->start - va_range->node.start;
+    loff_t offset;
+
+    struct kiocb kiocb;
+    struct iovec *iov = va_range->nvmgpu_va_range->iov;
+    struct iov_iter iter;
+    unsigned int iov_index = 0;
+    ssize_t _ret;
+
+    void *page_addr;
+
+    uvm_va_block_region_t region = uvm_va_block_region(0, (va_block->end - va_block->start + 1) / PAGE_SIZE);
+
+    uvm_page_mask_t mask;
+
+    UVM_ASSERT(nvmgpu_file != NULL);
+
+    if (!page_mask)
+        uvm_page_mask_fill(&mask);
+    else
+        uvm_page_mask_copy(&mask, page_mask);
+
+    // Switch the filesystem space to kernel space.
+    fs = get_fs();
+    set_fs(KERNEL_DS);
+
+    // Build iov based on the page addresses.
+    prev_page_id = -2;
+    offset = file_start_offset;
+    for_each_va_block_page_in_region_mask(page_id, &mask, region) {
+        if (!va_block->cpu.pages[page_id])
+            continue;
+
+        page_addr = page_address(va_block->cpu.pages[page_id]);
+
+        // Perform write.
+        if (page_id - 1 != prev_page_id && iov_index > 0) {
+            init_sync_kiocb(&kiocb, nvmgpu_file);
+            kiocb.ki_pos = offset;
+            iov_iter_init(&iter, WRITE, iov, iov_index, iov_index * PAGE_SIZE);
+            _ret = call_write_iter(nvmgpu_file, &kiocb, &iter);
+            BUG_ON(_ret == -EIOCBQUEUED);
+
+            iov_index = 0;
+            offset = file_start_offset + page_id * PAGE_SIZE;
+        }
+        iov[iov_index].iov_base = page_addr;
+        iov[iov_index].iov_len = PAGE_SIZE;
+        ++iov_index;
+        prev_page_id = page_id;
+    }
+
+    // Start write.
+    if (iov_index > 0) {
+        init_sync_kiocb(&kiocb, nvmgpu_file);
+        kiocb.ki_pos = offset;
+        iov_iter_init(&iter, WRITE, iov, iov_index, iov_index * PAGE_SIZE);
+        _ret = call_write_iter(nvmgpu_file, &kiocb, &iter);
+        BUG_ON(_ret == -EIOCBQUEUED);
+    }
+    
+    // Mark that this block has dirty data on the file.
+    uvm_nvmgpu_block_set_file_dirty(va_block);
+
+    // Switch back to the original space.
+    set_fs(fs);
+
+    return status;
+}
+
diff --git a/kernel/nvidia-uvm/uvm8_nvmgpu.h b/kernel/nvidia-uvm/uvm8_nvmgpu.h
new file mode 100644
index 0000000..1cf8e47
--- /dev/null
+++ b/kernel/nvidia-uvm/uvm8_nvmgpu.h
@@ -0,0 +1,310 @@
+#ifndef __UVM8_NVMGPU_H__
+#define __UVM8_NVMGPU_H__
+
+#include "uvm8_forward_decl.h"
+#include "uvm8_va_space.h"
+#include "uvm8_va_range.h"
+#include "uvm8_va_block.h"
+
+// Flags for each mapping
+#define UVM_NVMGPU_FLAG_READ        (1U << 1)
+#define UVM_NVMGPU_FLAG_WRITE       (1U << 2)
+#define UVM_NVMGPU_FLAG_DONTTRASH   (1U << 4)
+#define UVM_NVMGPU_FLAG_VOLATILE    (1U << 5)
+#define UVM_NVMGPU_FLAG_USEHOSTBUF  (1U << 6)
+#define UVM_NVMGPU_FLAG_DIRECT      (1U << 7)
+
+#define UVM_NVMGPU_GDIRECT_BUF_NUM_BLOCKS   1
+
+#define UVM_NVMGPU_PROFILING 0
+
+typedef struct uvm_nvmgpu_gdirect_struct uvm_nvmgpu_gdirect_t;
+
+typedef struct uvm_nvmgpu_gdirect_buffer_struct
+{
+    uvm_mutex_t lock;
+    uvm_nvmgpu_gdirect_t *gdirect;
+    struct bio_vec *bvecs;
+    unsigned int num_pages;
+    NvU64 aligned_rm_gpu_va;
+} uvm_nvmgpu_gdirect_buffer_t;
+
+struct uvm_nvmgpu_gdirect_struct
+{
+    bool p2pmem_support;
+
+    uvm_gpu_t *gpu;
+
+    uvm_rm_mem_t *rm_mem;
+    NvU64 aligned_rm_gpu_va;
+
+    unsigned long p2pmem_pfn;
+    void *p2pmem_va;
+    size_t p2pmem_size;
+    unsigned int p2pmem_num_pages;
+
+    struct bio_vec *bvecs;
+
+    unsigned int num_read_buffers;
+    uvm_nvmgpu_gdirect_buffer_t *read_buffers;
+    uvm_nvmgpu_gdirect_buffer_t *write_buffer;
+};
+
+struct uvm_nvmgpu_va_space_struct
+{
+    bool is_initailized;
+    bool gdirect_support;
+
+    // number of blocks to be trashed at a time
+    unsigned long trash_nr_blocks; 
+
+    // number of pages reserved for the system 
+    unsigned long trash_reserved_nr_pages; 
+
+    // init flags that dictate the optimization behaviors
+    unsigned short flags;
+
+    uvm_mutex_t lock;
+
+    struct list_head lru_head;
+
+    uvm_nvmgpu_gdirect_t gdirect_table[UVM_ID_MAX_GPUS];
+
+    #if UVM_NVMGPU_PROFILING
+    struct
+    {
+        NvU64 dio_read_file;
+        NvU64 dio_read_memcopy;
+
+        NvU64 pc_read_file;
+        NvU64 pc_read_memcopy;
+
+        NvU64 populate_gpu_pages;
+    } time;
+    #endif
+};
+
+typedef struct uvm_nvmgpu_va_range_struct
+{
+    struct file *filp;
+    unsigned int flags;
+    size_t size;
+    unsigned long *is_file_dirty_bitmaps;
+    unsigned long *has_data_bitmaps;
+    struct iovec *iov;
+} uvm_nvmgpu_va_range_t; 
+
+typedef enum
+{
+    UVM_NVMGPU_FILE_READ_TYPE_NONE = 0,
+    UVM_NVMGPU_FILE_READ_TYPE_PAGE_CACHE,
+    UVM_NVMGPU_FILE_READ_TYPE_DIRECT_IO
+} uvm_nvmgpu_file_read_type_t;
+
+#if UVM_NVMGPU_PROFILING
+static inline NvU64 uvm_nvmgpu_timediff(struct timespec start, struct timespec end)
+{
+    return (end.tv_sec - start.tv_sec) * 1000000000 + end.tv_nsec - start.tv_nsec;
+}
+#endif
+
+NV_STATUS uvm_nvmgpu_initialize(uvm_va_space_t *va_space, UVM_NVMGPU_INITIALIZE_PARAMS *params);
+void uvm_nvmgpu_deinitialize(uvm_va_space_t *va_space);
+
+NV_STATUS uvm_nvmgpu_register_file_va_space(uvm_va_space_t *va_space, UVM_NVMGPU_REGISTER_FILE_VA_SPACE_PARAMS *params);
+NV_STATUS uvm_nvmgpu_remap(uvm_va_space_t *va_space, UVM_NVMGPU_REMAP_PARAMS *params);
+NV_STATUS uvm_nvmgpu_unregister_va_range(uvm_va_range_t *va_range);
+
+NV_STATUS uvm_nvmgpu_flush_host_block(uvm_va_block_t *va_block, bool is_evict, const uvm_page_mask_t *page_mask);
+NV_STATUS uvm_nvmgpu_flush_block(uvm_va_block_t *va_block);
+NV_STATUS uvm_nvmgpu_flush(uvm_va_range_t *va_range);
+NV_STATUS uvm_nvmgpu_release_block(uvm_va_block_t *va_block);
+
+NV_STATUS uvm_nvmgpu_read_begin(uvm_va_block_t *va_block, uvm_va_block_retry_t *block_retry, uvm_service_block_context_t *service_context);
+NV_STATUS uvm_nvmgpu_read_end(uvm_va_block_t *va_block);
+
+NV_STATUS uvm_nvmgpu_write_begin(uvm_va_block_t *va_block, bool is_flush);
+NV_STATUS uvm_nvmgpu_write_end(uvm_va_block_t *va_block, bool is_flush);
+
+NV_STATUS uvm_nvmgpu_direct_io_read(
+    uvm_va_block_t *va_block, 
+    uvm_va_block_retry_t *block_retry, 
+    uvm_service_block_context_t *service_context,
+    uvm_make_resident_cause_t cause,
+    uvm_gpu_id_t gpu_id, 
+    const uvm_page_mask_t *page_mask
+);
+NV_STATUS uvm_nvmgpu_direct_io_write(uvm_va_block_t *va_block, uvm_page_mask_t *page_mask);
+
+NV_STATUS uvm_nvmgpu_reduce_memory_consumption(uvm_va_space_t *va_space);
+
+NV_STATUS uvm_nvmgpu_prepare_block_for_hostbuf(uvm_va_block_t *va_block);
+
+/**
+ * Has this va_space been initialized with nvmgpu driver?
+ *
+ * @param va_space: va_space to be examined.
+ * @return: true if this va_space has been initialized with nvmgpu driver, false otherwise.
+ */
+static inline bool uvm_nvmgpu_is_va_space_initialized(uvm_va_space_t *va_space)
+{
+    return va_space->nvmgpu_va_space && va_space->nvmgpu_va_space->is_initailized;
+}
+
+/**
+ * Is this va_range managed by nvmgpu driver?
+ *
+ * @param va_range: va_range to be examined.
+ * @return: true if this va_range is managed by nvmgpu driver, false otherwise.
+ */
+static inline bool uvm_nvmgpu_is_managed(uvm_va_range_t *va_range)
+{
+    return va_range->nvmgpu_va_range && va_range->nvmgpu_va_range->filp != NULL;
+}
+
+/**
+ * Determine if we need to reclaim some blocks or not.
+ *
+ * @param nvmgpu_va_space: the va_space information related to NVMGPU.
+ *
+ * @return: true if we need to reclaim, false otherwise.
+ */
+static inline bool uvm_nvmgpu_has_to_reclaim_blocks(uvm_nvmgpu_va_space_t *nvmgpu_va_space)
+{
+    unsigned long freeram = global_zone_page_state(NR_FREE_PAGES);
+    unsigned long pagecacheram = global_zone_page_state(NR_FILE_PAGES);
+    return freeram + pagecacheram < nvmgpu_va_space->trash_reserved_nr_pages;
+}
+
+static inline bool uvm_nvmgpu_block_file_dirty(uvm_va_block_t *va_block)
+{
+    uvm_va_range_t *va_range = va_block->va_range;
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = va_range->nvmgpu_va_range;
+
+    size_t index = uvm_va_range_block_index(va_range, va_block->start);
+    size_t list_index = index / BITS_PER_LONG;
+    size_t bitmap_index = index % BITS_PER_LONG;
+
+    return test_bit(bitmap_index, &nvmgpu_va_range->is_file_dirty_bitmaps[list_index]);
+}
+
+static inline uvm_nvmgpu_file_read_type_t uvm_nvmgpu_file_read_type(uvm_va_block_t *va_block, uvm_processor_id_t processor_id)
+{
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = va_block->va_range->nvmgpu_va_range;
+
+    // NVMGPU does not manage this va_range. 
+    // No need to read from file.
+    if (!uvm_nvmgpu_is_managed(va_block->va_range))
+        return UVM_NVMGPU_FILE_READ_TYPE_NONE;
+
+    // Data on this va_block has been written to storage via page-cache. 
+    // Read it back with the same method to avoid potential problems.
+    // TODO: Support read back with DIRECT_IO
+    if (uvm_nvmgpu_block_file_dirty(va_block))
+        return UVM_NVMGPU_FILE_READ_TYPE_PAGE_CACHE;
+
+    // We do not need to read from file for these following conditions.
+    if (
+        // No read flag
+        !(nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_READ)
+        // Volatile memory
+        || (nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_VOLATILE)
+        // Use CPU buffer and data is still on that buffer
+        || ((nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_USEHOSTBUF)
+            && va_block->nvmgpu.use_uvm_buffer
+        )
+    )
+        return UVM_NVMGPU_FILE_READ_TYPE_NONE;
+
+    // Request read with direct IO to GPU.
+    // We do not support direct IO for CPU access.
+    if (!UVM_ID_IS_CPU(processor_id) && (nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_DIRECT))
+        return UVM_NVMGPU_FILE_READ_TYPE_DIRECT_IO;
+
+    // Use page cache for everything else.
+    return UVM_NVMGPU_FILE_READ_TYPE_PAGE_CACHE;
+}
+
+static inline void uvm_nvmgpu_block_clear_has_data(uvm_va_block_t *va_block)
+{
+    uvm_va_range_t *va_range = va_block->va_range;
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = va_range->nvmgpu_va_range;
+
+    size_t index = uvm_va_range_block_index(va_range, va_block->start);
+    size_t list_index = index / BITS_PER_LONG;
+    size_t bitmap_index = index % BITS_PER_LONG;
+
+    clear_bit(bitmap_index, &nvmgpu_va_range->has_data_bitmaps[list_index]);
+}
+
+static inline void uvm_nvmgpu_block_set_has_data(uvm_va_block_t *va_block)
+{
+    uvm_va_range_t *va_range = va_block->va_range;
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = va_range->nvmgpu_va_range;
+
+    size_t index = uvm_va_range_block_index(va_range, va_block->start);
+    size_t list_index = index / BITS_PER_LONG;
+    size_t bitmap_index = index % BITS_PER_LONG;
+
+    set_bit(bitmap_index, &nvmgpu_va_range->has_data_bitmaps[list_index]);
+}
+
+static inline bool uvm_nvmgpu_block_has_data(uvm_va_block_t *va_block)
+{
+    uvm_va_range_t *va_range = va_block->va_range;
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = va_range->nvmgpu_va_range;
+
+    size_t index = uvm_va_range_block_index(va_range, va_block->start);
+    size_t list_index = index / BITS_PER_LONG;
+    size_t bitmap_index = index % BITS_PER_LONG;
+
+    return test_bit(bitmap_index, &nvmgpu_va_range->has_data_bitmaps[list_index]);
+}
+
+static inline void uvm_nvmgpu_block_clear_file_dirty(uvm_va_block_t *va_block)
+{
+    uvm_va_range_t *va_range = va_block->va_range;
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = va_range->nvmgpu_va_range;
+
+    size_t index = uvm_va_range_block_index(va_range, va_block->start);
+    size_t list_index = index / BITS_PER_LONG;
+    size_t bitmap_index = index % BITS_PER_LONG;
+
+    clear_bit(bitmap_index, &nvmgpu_va_range->is_file_dirty_bitmaps[list_index]);
+}
+
+static inline void uvm_nvmgpu_block_set_file_dirty(uvm_va_block_t *va_block)
+{
+    uvm_va_range_t *va_range = va_block->va_range;
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = va_range->nvmgpu_va_range;
+
+    size_t index = uvm_va_range_block_index(va_range, va_block->start);
+    size_t list_index = index / BITS_PER_LONG;
+    size_t bitmap_index = index % BITS_PER_LONG;
+
+    set_bit(bitmap_index, &nvmgpu_va_range->is_file_dirty_bitmaps[list_index]);
+}
+
+static inline bool uvm_nvmgpu_need_to_evict_from_gpu(uvm_va_block_t *va_block)
+{
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = va_block->va_range->nvmgpu_va_range;
+
+    return (nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_WRITE) || (nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_USEHOSTBUF);
+}
+
+/**
+ * Mark that we just touch this block, which has in-buffer data.
+ *
+ * @param va_block: va_block to be marked.
+ */
+static inline void uvm_nvmgpu_block_mark_recent_in_buffer(uvm_va_block_t *va_block)
+{
+    uvm_nvmgpu_va_space_t *nvmgpu_va_space = va_block->va_range->va_space->nvmgpu_va_space;
+    
+    // Move this block to the tail of the LRU list.
+    uvm_mutex_lock(&nvmgpu_va_space->lock);
+    list_move_tail(&va_block->nvmgpu.lru, &nvmgpu_va_space->lru_head);
+    uvm_mutex_unlock(&nvmgpu_va_space->lock);
+}
+
+#endif
diff --git a/kernel/nvidia-uvm/uvm8_va_block.c b/kernel/nvidia-uvm/uvm8_va_block.c
index 36a226d..f05ac43 100644
--- a/kernel/nvidia-uvm/uvm8_va_block.c
+++ b/kernel/nvidia-uvm/uvm8_va_block.c
@@ -38,6 +38,7 @@
 #include "uvm8_mem.h"
 #include "uvm8_gpu_access_counters.h"
 #include "uvm8_test_ioctl.h"
+#include "uvm8_nvmgpu.h"
 
 typedef enum
 {
@@ -532,6 +533,7 @@ NV_STATUS uvm_va_block_create(uvm_va_range_t *va_range,
         status = NV_ERR_NO_MEMORY;
         goto error;
     }
+    INIT_LIST_HEAD(&block->nvmgpu.lru);
 
     *out_block = block;
     return NV_OK;
@@ -871,8 +873,13 @@ static NV_STATUS block_populate_page_cpu(uvm_va_block_t *block, uvm_page_index_t
         gfp_flags |= __GFP_ZERO;
 
     page = alloc_pages(gfp_flags, 0);
-    if (!page)
-        return NV_ERR_NO_MEMORY;
+    if (!page) {
+        uvm_nvmgpu_reduce_memory_consumption(block->va_range->va_space);
+
+        page = alloc_pages(gfp_flags, 0);
+        if (!page)
+            return NV_ERR_NO_MEMORY;
+    }
 
     // the kernel has 'written' zeros to this page, so it is dirty
     if (zero)
@@ -883,6 +890,7 @@ static NV_STATUS block_populate_page_cpu(uvm_va_block_t *block, uvm_page_index_t
         goto error;
 
     block->cpu.pages[page_index] = page;
+    block->nvmgpu.use_uvm_buffer = true;
     return NV_OK;
 
 error:
@@ -1865,6 +1873,14 @@ static NV_STATUS block_populate_pages_gpu(uvm_va_block_t *block,
     uvm_chunk_size_t chunk_size;
     NV_STATUS status;
 
+    #if UVM_NVMGPU_PROFILING
+    uvm_nvmgpu_va_space_t *nvmgpu_va_space = block->va_range->va_space->nvmgpu_va_space;
+
+    struct timespec tv_start, tv_end;
+
+    getnstimeofday(&tv_start);
+    #endif
+
     page_index = uvm_va_block_first_page_in_mask(region, populate_mask);
     if (page_index == region.outer)
         return NV_OK;
@@ -1890,6 +1906,12 @@ static NV_STATUS block_populate_pages_gpu(uvm_va_block_t *block,
         chunk_region = uvm_va_block_region(chunk_region.outer, chunk_region.outer + (chunk_size / PAGE_SIZE));
     }
 
+    #if UVM_NVMGPU_PROFILING
+    getnstimeofday(&tv_end);
+
+    nvmgpu_va_space->time.populate_gpu_pages += uvm_nvmgpu_timediff(tv_start, tv_end);
+    #endif
+
     return NV_OK;
 }
 
@@ -1916,16 +1938,23 @@ static NV_STATUS block_populate_pages(uvm_va_block_t *block,
     if (UVM_ID_IS_GPU(dest_id))
         return block_populate_pages_gpu(block, retry, block_get_gpu(block, dest_id), region, populate_page_mask);
 
-    for_each_va_block_page_in_region_mask(page_index, populate_page_mask, region) {
-        uvm_processor_mask_t resident_on;
-        bool resident_somewhere;
-        uvm_va_block_page_resident_processors(block, page_index, &resident_on);
-        resident_somewhere = !uvm_processor_mask_empty(&resident_on);
-
-        // For pages not resident anywhere, need to populate with zeroed memory
-        status = block_populate_page_cpu(block, page_index, !resident_somewhere);
-        if (status != NV_OK)
-            return status;
+    if (!uvm_nvmgpu_is_managed(block->va_range)
+        || (!uvm_nvmgpu_block_file_dirty(block)
+            && ((block->va_range->nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_VOLATILE)
+                || (block->va_range->nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_USEHOSTBUF))
+        )
+    ) {
+        for_each_va_block_page_in_region_mask(page_index, populate_page_mask, region) {
+            uvm_processor_mask_t resident_on;
+            bool resident_somewhere;
+            uvm_va_block_page_resident_processors(block, page_index, &resident_on);
+            resident_somewhere = !uvm_processor_mask_empty(&resident_on);
+
+            // For pages not resident anywhere, need to populate with zeroed memory
+            status = block_populate_page_cpu(block, page_index, !resident_somewhere);
+            if (status != NV_OK)
+                return status;
+        }
     }
 
     return NV_OK;
@@ -2428,7 +2457,8 @@ static NV_STATUS block_copy_resident_pages_between(uvm_va_block_t *block,
             uvm_push_set_flag(&push, UVM_PUSH_FLAG_CE_NEXT_PIPELINED);
         }
 
-        block_update_page_dirty_state(block, dst_id, src_id, page_index);
+        if (!uvm_nvmgpu_is_managed(block->va_range))
+            block_update_page_dirty_state(block, dst_id, src_id, page_index);
 
         if (last_index == region.outer) {
             contig_start_index = page_index;
@@ -2748,6 +2778,7 @@ static NV_STATUS block_copy_resident_pages(uvm_va_block_t *block,
     NV_STATUS status = NV_OK;
     NV_STATUS tracker_status;
     uvm_tracker_t local_tracker = UVM_TRACKER_INIT();
+    uvm_make_resident_cause_t cause = block_context->make_resident.cause;
     uvm_page_mask_t *resident_mask = uvm_va_block_resident_mask_get(block, dst_id);
     NvU32 missing_pages_count;
     NvU32 pages_copied;
@@ -2835,20 +2866,43 @@ static NV_STATUS block_copy_resident_pages(uvm_va_block_t *block,
                                      BLOCK_TRANSFER_MODE_INTERNAL_MOVE_TO_STAGE;
     }
 
-    status = block_copy_resident_pages_mask(block,
-                                            block_context,
-                                            UVM_ID_CPU,
-                                            &src_processor_mask,
-                                            region,
-                                            copy_page_mask,
-                                            prefetch_page_mask,
-                                            transfer_mode_internal,
-                                            missing_pages_count,
-                                            staged_pages,
-                                            &pages_copied_to_cpu,
-                                            &local_tracker);
-    if (status != NV_OK)
-        goto out;
+    if (uvm_nvmgpu_is_managed(block->va_range)
+        && (cause == UVM_MAKE_RESIDENT_CAUSE_EVICTION)
+        && uvm_nvmgpu_need_to_evict_from_gpu(block)
+        && (block->va_range->nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_DIRECT)
+    ) {
+        uvm_page_mask_t write_page_mask;
+        uvm_page_mask_copy(&write_page_mask, copy_page_mask);
+
+        // For eviction with DIRECT IO, we skip host buffer all together.
+        status = uvm_nvmgpu_direct_io_write(block, &write_page_mask);
+        if (status != NV_OK) {
+            printk(KERN_DEBUG "uvm_nvmgpu_direct_io_write error %s\n", nvstatusToString(status));
+            goto out;
+        }
+    }
+    else if (!uvm_nvmgpu_is_managed(block->va_range)
+        || (cause != UVM_MAKE_RESIDENT_CAUSE_EVICTION && cause != UVM_MAKE_RESIDENT_CAUSE_API_MIGRATE && cause != UVM_MAKE_RESIDENT_CAUSE_NVMGPU) 
+        || (cause == UVM_MAKE_RESIDENT_CAUSE_EVICTION && uvm_nvmgpu_need_to_evict_from_gpu(block)) 
+        || (cause == UVM_MAKE_RESIDENT_CAUSE_API_MIGRATE && UVM_ID_IS_CPU(dst_id) && uvm_nvmgpu_need_to_evict_from_gpu(block))
+        || (cause == UVM_MAKE_RESIDENT_CAUSE_NVMGPU && UVM_ID_IS_GPU(dst_id))
+    ) {
+        // Do it if we really need to copy data to host buffer.
+        status = block_copy_resident_pages_mask(block,
+                                                block_context,
+                                                UVM_ID_CPU,
+                                                &src_processor_mask,
+                                                region,
+                                                copy_page_mask,
+                                                prefetch_page_mask,
+                                                transfer_mode_internal,
+                                                missing_pages_count,
+                                                staged_pages,
+                                                &pages_copied_to_cpu,
+                                                &local_tracker);
+        if (status != NV_OK)
+            goto out;
+    }
 
     // If destination is the CPU then we copied everything there above
     if (UVM_ID_IS_CPU(dst_id)) {
@@ -2946,6 +3000,23 @@ NV_STATUS uvm_va_block_make_resident(uvm_va_block_t *va_block,
     uvm_page_mask_t *unmap_page_mask = &va_block_context->make_resident.page_mask;
     uvm_page_mask_t *resident_mask;
 
+    bool do_nvmgpu_write = false;
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range = NULL;
+
+    #if UVM_NVMGPU_PROFILING
+    uvm_nvmgpu_va_space_t *nvmgpu_va_space = NULL;
+
+    struct timespec tv_start = {0}, tv_end = {0};
+    #endif
+
+    if (uvm_nvmgpu_is_managed(va_range)) {
+        nvmgpu_va_range = va_range->nvmgpu_va_range;
+
+        #if UVM_NVMGPU_PROFILING
+        nvmgpu_va_space = va_range->va_space->nvmgpu_va_space;
+        #endif
+    }
+
     va_block_context->make_resident.dest_id = dest_id;
     va_block_context->make_resident.cause = cause;
 
@@ -2959,6 +3030,25 @@ NV_STATUS uvm_va_block_make_resident(uvm_va_block_t *va_block,
     UVM_ASSERT(va_block->va_range);
     UVM_ASSERT(va_block->va_range->type == UVM_VA_RANGE_TYPE_MANAGED);
 
+    if (uvm_nvmgpu_is_managed(va_range)
+        && uvm_nvmgpu_need_to_evict_from_gpu(va_block)
+        && (cause == UVM_MAKE_RESIDENT_CAUSE_EVICTION 
+            || (cause == UVM_MAKE_RESIDENT_CAUSE_API_MIGRATE && UVM_ID_IS_CPU(dest_id))) 
+        && !(nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_DIRECT)
+    ) {
+        if (!uvm_nvmgpu_block_file_dirty(va_block)
+            && ((nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_VOLATILE)
+                || (nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_USEHOSTBUF))
+        ) {
+            if (nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_USEHOSTBUF)
+                uvm_nvmgpu_prepare_block_for_hostbuf(va_block);
+            uvm_nvmgpu_block_mark_recent_in_buffer(va_block);
+        }
+        else {
+            uvm_nvmgpu_write_begin(va_block, cause == UVM_MAKE_RESIDENT_CAUSE_API_MIGRATE);
+            do_nvmgpu_write = true;
+        }
+    }
     resident_mask = block_resident_mask_get_alloc(va_block, dest_id);
     if (!resident_mask)
         return NV_ERR_NO_MEMORY;
@@ -2997,9 +3087,22 @@ NV_STATUS uvm_va_block_make_resident(uvm_va_block_t *va_block,
     // va_block_context->make_resident.page_mask.
     unmap_page_mask = NULL;
 
-    status = block_populate_pages(va_block, va_block_retry, va_block_context, dest_id, region, page_mask);
-    if (status != NV_OK)
-        return status;
+    // For NVMGPU with DIRECT IO in the eviction path, we don't need to populate CPU pages.
+    if (!(
+        uvm_nvmgpu_is_managed(va_range)
+        && (cause == UVM_MAKE_RESIDENT_CAUSE_EVICTION)
+        && (nvmgpu_va_range->flags & UVM_NVMGPU_FLAG_DIRECT)
+    )) {
+        status = block_populate_pages(va_block, va_block_retry, va_block_context, dest_id, region, page_mask);
+        if (status != NV_OK)
+            return status;
+    }
+
+
+    #if UVM_NVMGPU_PROFILING
+    if (uvm_nvmgpu_is_managed(va_range))
+        getnstimeofday(&tv_start);
+    #endif
 
     status = block_copy_resident_pages(va_block,
                                        va_block_context,
@@ -3011,6 +3114,13 @@ NV_STATUS uvm_va_block_make_resident(uvm_va_block_t *va_block,
     if (status != NV_OK)
         return status;
 
+    #if UVM_NVMGPU_PROFILING
+    if (uvm_nvmgpu_is_managed(va_range)) {
+        getnstimeofday(&tv_end);
+        nvmgpu_va_space->time.pc_read_memcopy += uvm_nvmgpu_timediff(tv_start, tv_end);
+    }
+    #endif
+
     // Update eviction heuristics, if needed. Notably this could repeat the call
     // done in block_set_resident_processor(), but that doesn't do anything bad
     // and it's simpler to keep it in both places.
@@ -3019,6 +3129,12 @@ NV_STATUS uvm_va_block_make_resident(uvm_va_block_t *va_block,
     // empty).
     if (uvm_processor_mask_test(&va_block->resident, dest_id))
         block_mark_memory_used(va_block, dest_id);
+    if (do_nvmgpu_write) {
+        status = uvm_tracker_wait(&va_block->tracker);
+        uvm_nvmgpu_write_end(va_block, cause == UVM_MAKE_RESIDENT_CAUSE_API_MIGRATE);
+        if (status != NV_OK)
+            return status;
+    }
 
     return NV_OK;
 }
@@ -3866,9 +3982,10 @@ static void block_unmap_cpu(uvm_va_block_t *block, uvm_va_block_region_t region,
         if (!block_has_valid_mapping_cpu(block, subregion))
             continue;
 
-        unmap_mapping_range(&va_range->va_space->mapping,
-                            uvm_va_block_region_start(block, subregion),
-                            uvm_va_block_region_size(subregion), 1);
+        if (!uvm_nvmgpu_is_managed(va_range))
+            unmap_mapping_range(&va_range->va_space->mapping,
+                                uvm_va_block_region_start(block, subregion),
+                                uvm_va_block_region_size(subregion), 1);
 
         for (pte_bit = 0; pte_bit < UVM_PTE_BITS_CPU_MAX; pte_bit++)
             uvm_page_mask_region_clear(&block->cpu.pte_bits[pte_bit], subregion);
@@ -7755,17 +7872,20 @@ static void block_kill(uvm_va_block_t *block)
 
     // Free CPU pages
     if (block->cpu.pages) {
-        uvm_page_index_t page_index;
-        for_each_va_block_page(page_index, block) {
-            if (block->cpu.pages[page_index]) {
-                // be conservative.
-                // Tell the OS we wrote to the page because we sometimes clear the dirty bit after writing to it.
-                SetPageDirty(block->cpu.pages[page_index]);
-                __free_page(block->cpu.pages[page_index]);
-            }
-            else {
-                UVM_ASSERT(!uvm_page_mask_test(&block->cpu.resident, page_index));
+        if (block->nvmgpu.use_uvm_buffer) {
+            uvm_page_index_t page_index;
+            for_each_va_block_page(page_index, block) {
+                if (block->cpu.pages[page_index]) {
+                    // be conservative.
+                    // Tell the OS we wrote to the page because we sometimes clear the dirty bit after writing to it.
+                    SetPageDirty(block->cpu.pages[page_index]);
+                    __free_page(block->cpu.pages[page_index]);
+                }
+                else {
+                    UVM_ASSERT(!uvm_page_mask_test(&block->cpu.resident, page_index));
+                }
             }
+            block->nvmgpu.use_uvm_buffer = false;
         }
 
         // Clearing the resident bit isn't strictly necessary since this block
@@ -9426,6 +9546,7 @@ NV_STATUS uvm_va_block_service_locked(uvm_processor_id_t processor_id,
     uvm_perf_prefetch_hint_t prefetch_hint = UVM_PERF_PREFETCH_HINT_NONE();
     uvm_processor_mask_t processors_involved_in_cpu_migration;
 
+    bool do_nvmgpu_page_cache_read = false;
     uvm_assert_mutex_locked(&va_block->lock);
     UVM_ASSERT(va_range->type == UVM_VA_RANGE_TYPE_MANAGED);
 
@@ -9525,18 +9646,46 @@ NV_STATUS uvm_va_block_service_locked(uvm_processor_id_t processor_id,
             uvm_page_mask_andnot(&service_context->block_context.caller_page_mask,
                                  new_residency_mask,
                                  &service_context->read_duplicate_mask)) {
-            status = uvm_va_block_make_resident(va_block,
-                                                block_retry,
-                                                &service_context->block_context,
-                                                new_residency,
-                                                service_context->region,
-                                                service_context->read_duplicate_count == 0?
-                                                    new_residency_mask:
-                                                    &service_context->block_context.caller_page_mask,
-                                                prefetch_hint.prefetch_pages_mask,
-                                                cause);
+
+            // Get the NVMGPU file read type.
+            uvm_nvmgpu_file_read_type_t file_read_type = uvm_nvmgpu_file_read_type(va_block, new_residency);
+
+            // Use page cache for read.
+            if (file_read_type == UVM_NVMGPU_FILE_READ_TYPE_PAGE_CACHE) {
+                status = uvm_nvmgpu_read_begin(va_block, block_retry, service_context);
+                if (status != NV_OK)
+                    goto error;
+                do_nvmgpu_page_cache_read = true;
+            }
+
+            // This va_block uses host buffer and is being touch. Move it to
+            // the LRU's tail so that we won't evict it too soon.
+            if (uvm_nvmgpu_is_managed(va_block->va_range) 
+                && (va_block->nvmgpu.use_uvm_buffer)
+            )
+                uvm_nvmgpu_block_mark_recent_in_buffer(va_block);
+
+            if (file_read_type == UVM_NVMGPU_FILE_READ_TYPE_DIRECT_IO)
+                // Use direct IO for read.
+                status = uvm_nvmgpu_direct_io_read(
+                    va_block, block_retry, service_context, cause, 
+                    new_residency, new_residency_mask
+                );
+            else
+                // Original UVM code.
+                // For read type page cache, it copies data to GPU.
+                status = uvm_va_block_make_resident(va_block,
+                                                    block_retry,
+                                                    &service_context->block_context,
+                                                    new_residency,
+                                                    service_context->region,
+                                                    service_context->read_duplicate_count == 0?
+                                                        new_residency_mask:
+                                                        &service_context->block_context.caller_page_mask,
+                                                    prefetch_hint.prefetch_pages_mask,
+                                                    cause);
             if (status != NV_OK)
-                return status;
+                goto error;
         }
 
         if (service_context->read_duplicate_count != 0 &&
@@ -9552,7 +9701,7 @@ NV_STATUS uvm_va_block_service_locked(uvm_processor_id_t processor_id,
                                                                prefetch_hint.prefetch_pages_mask,
                                                                cause);
             if (status != NV_OK)
-                return status;
+                goto error;
         }
 
         if (UVM_ID_IS_CPU(new_residency)) {
@@ -9657,7 +9806,7 @@ NV_STATUS uvm_va_block_service_locked(uvm_processor_id_t processor_id,
                                                        &service_context->revocation_mask,
                                                        revoke_prot);
                 if (status != NV_OK)
-                    return status;
+                    goto error;
             }
         }
     }
@@ -9696,7 +9845,7 @@ NV_STATUS uvm_va_block_service_locked(uvm_processor_id_t processor_id,
                 status = NV_OK;
             }
             if (status != NV_OK)
-                return status;
+                goto error;
         }
     }
 
@@ -9722,7 +9871,7 @@ NV_STATUS uvm_va_block_service_locked(uvm_processor_id_t processor_id,
                                         map_prot_mask,
                                         NULL);
             if (status != NV_OK)
-                return status;
+                goto error;
         }
 
         // 3.2 - Add new mappings
@@ -9747,7 +9896,7 @@ NV_STATUS uvm_va_block_service_locked(uvm_processor_id_t processor_id,
                                           UvmEventMapRemoteCauseThrashing,
                                           &va_block->tracker);
                 if (status != NV_OK)
-                    return status;
+                    goto error;
 
                 // Remove thrashing pages from the map mask
                 pages_need_mapping = uvm_page_mask_andnot(helper_page_mask,
@@ -9769,7 +9918,7 @@ NV_STATUS uvm_va_block_service_locked(uvm_processor_id_t processor_id,
                                   UvmEventMapRemoteCausePolicy,
                                   &va_block->tracker);
         if (status != NV_OK)
-            return status;
+            goto error;
     }
 
     // 4- If pages did migrate, map SetAccessedBy processors, except for UVM-Lite
@@ -9815,7 +9964,7 @@ NV_STATUS uvm_va_block_service_locked(uvm_processor_id_t processor_id,
                                                                        new_prot,
                                                                        map_thrashing_processors);
                     if (status != NV_OK)
-                        return status;
+                        goto error;
                 }
 
                 pages_need_mapping = uvm_page_mask_andnot(map_prot_mask,
@@ -9835,11 +9984,23 @@ NV_STATUS uvm_va_block_service_locked(uvm_processor_id_t processor_id,
                                                                new_prot,
                                                                NULL);
             if (status != NV_OK)
-                return status;
+                goto error;
         }
     }
 
-    return NV_OK;
+error:
+    if (do_nvmgpu_page_cache_read) {
+        status = uvm_tracker_wait(&va_block->tracker);
+
+        uvm_nvmgpu_read_end(va_block);
+
+        if (UVM_ID_IS_CPU(processor_id)) {
+            uvm_nvmgpu_write_begin(va_block, false);
+            uvm_nvmgpu_write_end(va_block, false);
+        }
+    }
+
+    return status;
 }
 
 // Check if we are faulting on a page with valid permissions to check if we can
diff --git a/kernel/nvidia-uvm/uvm8_va_block.h b/kernel/nvidia-uvm/uvm8_va_block.h
index bca1c6e..0386440 100644
--- a/kernel/nvidia-uvm/uvm8_va_block.h
+++ b/kernel/nvidia-uvm/uvm8_va_block.h
@@ -452,6 +452,11 @@ struct uvm_va_block_struct
     nv_kthread_q_item_t eviction_mappings_q_item;
 
     uvm_perf_module_data_desc_t perf_modules_data[UVM_PERF_MODULE_TYPE_COUNT];
+
+    struct {
+        bool use_uvm_buffer;
+        struct list_head lru;
+    } nvmgpu;
 };
 
 // We define additional per-VA Block fields for testing. When
@@ -1347,6 +1352,11 @@ static void uvm_page_mask_zero(uvm_page_mask_t *mask)
     bitmap_zero(mask->bitmap, PAGES_PER_UVM_VA_BLOCK);
 }
 
+static void uvm_page_mask_fill(uvm_page_mask_t *mask)
+{
+    bitmap_fill(mask->bitmap, PAGES_PER_UVM_VA_BLOCK);
+}
+
 static bool uvm_page_mask_empty(const uvm_page_mask_t *mask)
 {
     return bitmap_empty(mask->bitmap, PAGES_PER_UVM_VA_BLOCK);
diff --git a/kernel/nvidia-uvm/uvm8_va_block_types.h b/kernel/nvidia-uvm/uvm8_va_block_types.h
index b0642dc..2679ad2 100644
--- a/kernel/nvidia-uvm/uvm8_va_block_types.h
+++ b/kernel/nvidia-uvm/uvm8_va_block_types.h
@@ -147,6 +147,7 @@ typedef enum
     UVM_MAKE_RESIDENT_CAUSE_API_MIGRATE,
     UVM_MAKE_RESIDENT_CAUSE_API_SET_RANGE_GROUP,
     UVM_MAKE_RESIDENT_CAUSE_API_HINT,
+    UVM_MAKE_RESIDENT_CAUSE_NVMGPU,
 
     UVM_MAKE_RESIDENT_CAUSE_MAX
 } uvm_make_resident_cause_t;
diff --git a/kernel/nvidia-uvm/uvm8_va_range.h b/kernel/nvidia-uvm/uvm8_va_range.h
index 49b3b43..d998ee7 100644
--- a/kernel/nvidia-uvm/uvm8_va_range.h
+++ b/kernel/nvidia-uvm/uvm8_va_range.h
@@ -344,6 +344,8 @@ struct uvm_va_range_struct
         uvm_va_range_sked_reflected_t sked_reflected;
         uvm_va_range_semaphore_pool_t semaphore_pool;
     };
+
+    uvm_nvmgpu_va_range_t *nvmgpu_va_range;
 };
 
 // Module load/exit
diff --git a/kernel/nvidia-uvm/uvm8_va_space.c b/kernel/nvidia-uvm/uvm8_va_space.c
index 74f9b20..f900f3d 100644
--- a/kernel/nvidia-uvm/uvm8_va_space.c
+++ b/kernel/nvidia-uvm/uvm8_va_space.c
@@ -40,6 +40,8 @@
 #include "nv_uvm_interface.h"
 #include "nv-kthread-q.h"
 
+#include "uvm8_nvmgpu.h"
+
 static bool processor_mask_array_test(const uvm_processor_mask_t *mask,
                                       uvm_processor_id_t mask_id,
                                       uvm_processor_id_t id)
@@ -396,6 +398,9 @@ void uvm_va_space_destroy(uvm_va_space_t *va_space)
 
     uvm_range_group_radix_tree_destroy(va_space);
 
+    if (uvm_nvmgpu_is_va_space_initialized(va_space))
+        uvm_nvmgpu_deinitialize(va_space);
+
     // Unregister all GPUs in the VA space. Note that this does not release the
     // GPUs nor peers. We do that below.
     for_each_va_space_gpu(gpu, va_space)
diff --git a/kernel/nvidia-uvm/uvm8_va_space.h b/kernel/nvidia-uvm/uvm8_va_space.h
index ae7b565..900c287 100644
--- a/kernel/nvidia-uvm/uvm8_va_space.h
+++ b/kernel/nvidia-uvm/uvm8_va_space.h
@@ -384,6 +384,8 @@ struct uvm_va_space_struct
 
     // Queue item for deferred f_ops->release() handling
     nv_kthread_q_item_t deferred_release_q_item;
+
+    uvm_nvmgpu_va_space_t *nvmgpu_va_space;
 };
 
 static uvm_gpu_t *uvm_va_space_get_gpu(uvm_va_space_t *va_space, uvm_gpu_id_t gpu_id)
diff --git a/kernel/nvidia-uvm/uvm_common.h b/kernel/nvidia-uvm/uvm_common.h
index 1d052f3..4969a6e 100644
--- a/kernel/nvidia-uvm/uvm_common.h
+++ b/kernel/nvidia-uvm/uvm_common.h
@@ -46,6 +46,7 @@
 enum {
     NVIDIA_UVM_PRIMARY_MINOR_NUMBER = 0,
     NVIDIA_UVM_TOOLS_MINOR_NUMBER   = 1,
+    NVIDIA_UVM_NVMGPU_MINOR_NUMBER  = 2,
     // to ensure backward-compatiblity and correct counting, please insert any
     // new minor devices just above the following field:
     NVIDIA_UVM_NUM_MINOR_DEVICES
diff --git a/kernel/nvidia-uvm/uvm_ioctl.h b/kernel/nvidia-uvm/uvm_ioctl.h
index 069a9f2..b63a6a6 100644
--- a/kernel/nvidia-uvm/uvm_ioctl.h
+++ b/kernel/nvidia-uvm/uvm_ioctl.h
@@ -23,6 +23,7 @@
 #ifndef _UVM_IOCTL_H
 #define _UVM_IOCTL_H
 
+#include <linux/types.h>
 #include "uvmtypes.h"
 
 #ifdef __cplusplus
@@ -1038,6 +1039,48 @@ typedef struct
     NV_STATUS       rmStatus;                    // OUT
 } UVM_VALIDATE_VA_RANGE_PARAMS;
 
+//
+// UvmNvmgpuInitialize
+//
+#define UVM_NVMGPU_INITIALIZE                                         UVM_IOCTL_BASE(1000)
+
+typedef struct
+{
+    unsigned long    trash_nr_blocks;           // IN
+    unsigned long    trash_reserved_nr_pages;   // IN
+    unsigned int     gdirect_num_groups;        // IN
+    unsigned short   flags;                     // IN
+    NV_STATUS        rmStatus;                  // OUT
+} UVM_NVMGPU_INITIALIZE_PARAMS;
+
+//
+// UvmNvmgpuRegisterFileVaSpace
+//
+#define UVM_NVMGPU_REGISTER_FILE_VA_SPACE                             UVM_IOCTL_BASE(1001)
+
+typedef struct
+{
+    int             backing_fd;         // IN
+    NvU64           uvm_addr;           // IN
+    size_t          size;               // IN
+    unsigned int    flags;              // IN
+    NV_STATUS       rmStatus;           // OUT
+} UVM_NVMGPU_REGISTER_FILE_VA_SPACE_PARAMS;
+
+//
+// UvmNvmgpuRemap
+//
+#define UVM_NVMGPU_REMAP                                              UVM_IOCTL_BASE(1004)
+
+typedef struct
+{
+    int             backing_fd;         // IN
+    NvU64           uvm_addr;           // IN
+    size_t          size;               // IN
+    unsigned int    flags;              // IN
+    NV_STATUS       rmStatus;           // OUT
+} UVM_NVMGPU_REMAP_PARAMS;                          
+
 //
 // Temporary ioctls which should be removed before UVM 8 release
 // Number backwards from 2047 - highest custom ioctl function number
