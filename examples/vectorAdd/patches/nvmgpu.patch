diff -Naru /home/pak/Projects/cuda-samples/0_Simple/vectorAdd/Makefile ../programs/nvmgpu/Makefile
--- /home/pak/Projects/cuda-samples/0_Simple/vectorAdd/Makefile	2019-07-21 13:09:19.969354274 +0700
+++ ../programs/nvmgpu/Makefile	2020-05-04 11:16:32.697403153 +0700
@@ -34,7 +34,7 @@
 ################################################################################
 
 # Location of the CUDA Toolkit
-CUDA_PATH ?= /usr/local/cuda-10.0
+CUDA_PATH ?= "/usr/local/cuda"
 
 ##############################
 # start deprecated interface #
@@ -141,7 +141,7 @@
             export QNX_TARGET
             HOST_COMPILER ?= $(QNX_HOST)/usr/bin/aarch64-unknown-nto-qnx7.0.0-g++
         else ifeq ($(TARGET_OS), android)
-            HOST_COMPILER ?= aarch64-linux-android-clang++
+            HOST_COMPILER ?= aarch64-linux-android-g++
         endif
     else ifeq ($(TARGET_ARCH),ppc64le)
         HOST_COMPILER ?= powerpc64le-linux-gnu-g++
@@ -180,44 +180,6 @@
             LDFLAGS += -rpath-link=$(TARGET_FS)/usr/lib/arm-linux-gnueabihf
         endif
     endif
-    ifeq ($(TARGET_ARCH)-$(TARGET_OS),aarch64-linux)
-        ifneq ($(TARGET_FS),)
-            GCCVERSIONLTEQ46 := $(shell expr `$(HOST_COMPILER) -dumpversion` \<= 4.6)
-            ifeq ($(GCCVERSIONLTEQ46),1)
-                CCFLAGS += --sysroot=$(TARGET_FS)
-            endif
-            LDFLAGS += --sysroot=$(TARGET_FS)
-            LDFLAGS += -rpath-link=$(TARGET_FS)/lib -L $(TARGET_FS)/lib
-            LDFLAGS += -rpath-link=$(TARGET_FS)/usr/lib -L $(TARGET_FS)/usr/lib
-            LDFLAGS += -rpath-link=$(TARGET_FS)/usr/lib/aarch64-linux-gnu -L $(TARGET_FS)/usr/lib/aarch64-linux-gnu
-            LDFLAGS += --unresolved-symbols=ignore-in-shared-libs
-            CCFLAGS += -isystem=$(TARGET_FS)/usr/include
-            CCFLAGS += -isystem=$(TARGET_FS)/usr/include/aarch64-linux-gnu
-        endif
-    endif
-endif
-
-ifeq ($(TARGET_OS),qnx)
-    CCFLAGS += -DWIN_INTERFACE_CUSTOM
-    LDFLAGS += -lsocket
-endif
-
-# Install directory of different arch
-CUDA_INSTALL_TARGET_DIR :=
-ifeq ($(TARGET_ARCH)-$(TARGET_OS),armv7l-linux)
-    CUDA_INSTALL_TARGET_DIR = targets/armv7-linux-gnueabihf/
-else ifeq ($(TARGET_ARCH)-$(TARGET_OS),aarch64-linux)
-    CUDA_INSTALL_TARGET_DIR = targets/aarch64-linux/
-else ifeq ($(TARGET_ARCH)-$(TARGET_OS),armv7l-android)
-    CUDA_INSTALL_TARGET_DIR = targets/armv7-linux-androideabi/
-else ifeq ($(TARGET_ARCH)-$(TARGET_OS),aarch64-android)
-    CUDA_INSTALL_TARGET_DIR = targets/aarch64-linux-androideabi/
-else ifeq ($(TARGET_ARCH)-$(TARGET_OS),armv7l-qnx)
-    CUDA_INSTALL_TARGET_DIR = targets/ARMv7-linux-QNX/
-else ifeq ($(TARGET_ARCH)-$(TARGET_OS),aarch64-qnx)
-    CUDA_INSTALL_TARGET_DIR = targets/aarch64-qnx/
-else ifeq ($(TARGET_ARCH),ppc64le)
-    CUDA_INSTALL_TARGET_DIR = targets/ppc64le-linux/
 endif
 
 # Debug build flags
@@ -242,13 +204,13 @@
 ALL_LDFLAGS += $(addprefix -Xlinker ,$(EXTRA_LDFLAGS))
 
 # Common includes and paths for CUDA
-INCLUDES  := -I../../common/inc
+INCLUDES  := -I$(CUDA_PATH)/samples/common/inc
 LIBRARIES :=
 
 ################################################################################
 
 # Gencode arguments
-SMS ?= 30 35 37 50 52 60 61 70 75
+SMS ?= 75
 
 ifeq ($(SMS),)
 $(info >>> WARNING - no SM architectures have been specified - waiving sample <<<)
@@ -288,15 +250,15 @@
 	$(EXEC) $(NVCC) $(INCLUDES) $(ALL_CCFLAGS) $(GENCODE_FLAGS) -o $@ -c $<
 
 vectorAdd: vectorAdd.o
-	$(EXEC) $(NVCC) $(ALL_LDFLAGS) $(GENCODE_FLAGS) -o $@ $+ $(LIBRARIES)
-	$(EXEC) mkdir -p ../../bin/$(TARGET_ARCH)/$(TARGET_OS)/$(BUILD_TYPE)
-	$(EXEC) cp $@ ../../bin/$(TARGET_ARCH)/$(TARGET_OS)/$(BUILD_TYPE)
+	$(EXEC) $(NVCC) $(ALL_LDFLAGS) $(GENCODE_FLAGS) -o $@ $+ $(LIBRARIES) -ldragon
+	$(EXEC) mkdir -p bin/
+	$(EXEC) mv $@ bin/
 
 run: build
 	$(EXEC) ./vectorAdd
 
 clean:
 	rm -f vectorAdd vectorAdd.o
-	rm -rf ../../bin/$(TARGET_ARCH)/$(TARGET_OS)/$(BUILD_TYPE)/vectorAdd
+	rm -rf bin/
 
 clobber: clean
diff -Naru /home/pak/Projects/cuda-samples/0_Simple/vectorAdd/NsightEclipse.xml ../programs/nvmgpu/NsightEclipse.xml
--- /home/pak/Projects/cuda-samples/0_Simple/vectorAdd/NsightEclipse.xml	2019-07-21 13:09:19.969354274 +0700
+++ ../programs/nvmgpu/NsightEclipse.xml	2020-03-01 12:56:36.072836165 +0700
@@ -37,15 +37,13 @@
     <scope>1:CUDA Basic Topics</scope>
     <scope>3:Linear Algebra</scope>
   </scopes>
+  <sm-arch>sm20</sm-arch>
   <sm-arch>sm30</sm-arch>
   <sm-arch>sm35</sm-arch>
   <sm-arch>sm37</sm-arch>
   <sm-arch>sm50</sm-arch>
   <sm-arch>sm52</sm-arch>
   <sm-arch>sm60</sm-arch>
-  <sm-arch>sm61</sm-arch>
-  <sm-arch>sm70</sm-arch>
-  <sm-arch>sm75</sm-arch>
   <supported_envs>
     <env>
       <arch>x86_64</arch>
diff -Naru /home/pak/Projects/cuda-samples/0_Simple/vectorAdd/readme.txt ../programs/nvmgpu/readme.txt
--- /home/pak/Projects/cuda-samples/0_Simple/vectorAdd/readme.txt	2019-07-21 13:09:19.969354274 +0700
+++ ../programs/nvmgpu/readme.txt	2020-03-01 12:56:36.072836165 +0700
@@ -1,5 +1,5 @@
 Sample: vectorAdd
-Minimum spec: SM 3.0
+Minimum spec: SM 2.0
 
 This CUDA Runtime API sample is a very basic sample that implements element by element vector addition. It is the same as the sample illustrating Chapter 3 of the programming guide with some additions like error checking.
 
diff -Naru /home/pak/Projects/cuda-samples/0_Simple/vectorAdd/run.sh ../programs/nvmgpu/run.sh
--- /home/pak/Projects/cuda-samples/0_Simple/vectorAdd/run.sh	1970-01-01 07:00:00.000000000 +0700
+++ ../programs/nvmgpu/run.sh	2020-03-01 12:56:36.072836165 +0700
@@ -0,0 +1,18 @@
+#!/bin/bash
+
+threads_per_block=1024
+
+for step in {1..3}
+do
+    #for size_in_gib in {1..12}
+    for size_in_gib in {13..16}
+    do
+        echo "==> step,size_in_gib,tpb: $step,$size_in_gib,$threads_per_block"
+        dmesg -C
+        ../../../experiments/drop-caches.sh
+        ./bin/vectorAdd $size_in_gib $threads_per_block
+        dmesg -l debug
+        echo "========================="
+    done
+done
+
diff -Naru /home/pak/Projects/cuda-samples/0_Simple/vectorAdd/vectorAdd.cu ../programs/nvmgpu/vectorAdd.cu
--- /home/pak/Projects/cuda-samples/0_Simple/vectorAdd/vectorAdd.cu	2019-07-21 13:09:19.969354274 +0700
+++ ../programs/nvmgpu/vectorAdd.cu	2020-05-04 22:00:07.168614029 +0700
@@ -18,11 +18,43 @@
  */
 
 #include <stdio.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include <unistd.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <sys/time.h>
 
 // For the CUDA runtime routines (prefixed with "cuda_")
 #include <cuda_runtime.h>
 
 #include <helper_cuda.h>
+
+#include <dragon.h>
+
+#define CUDA_CALL_SAFE(f) \
+    do \
+    {                                                        \
+        cudaError_t _cuda_error = f;                         \
+        if (_cuda_error != cudaSuccess)                      \
+        {                                                    \
+            fprintf(stderr,  \
+                "%s, %d, CUDA ERROR: %s %s\n",  \
+                __FILE__,   \
+                __LINE__,   \
+                cudaGetErrorName(_cuda_error),  \
+                cudaGetErrorString(_cuda_error) \
+            ); \
+            abort(); \
+            return EXIT_FAILURE; \
+        } \
+    } while (0)        
+
+double time_diff(struct timeval tv_start, struct timeval tv_stop)
+{
+    return (double)(tv_stop.tv_sec - tv_start.tv_sec) * 1000.0 + (double)(tv_stop.tv_usec - tv_start.tv_usec) / 1000.0;
+}
 /**
  * CUDA Kernel Device code
  *
@@ -30,9 +62,9 @@
  * number of elements numElements.
  */
 __global__ void
-vectorAdd(const float *A, const float *B, float *C, int numElements)
+vectorAdd(const float *A, const float *B, float *C, unsigned long numElements)
 {
-    int i = blockDim.x * blockIdx.x + threadIdx.x;
+    unsigned long i = (unsigned long)blockDim.x * (unsigned long)blockIdx.x + (unsigned long)threadIdx.x;
 
     if (i < numElements)
     {
@@ -43,94 +75,93 @@
 /**
  * Host main routine
  */
-int
-main(void)
+int main(int argc, char *argv[])
 {
     // Error code to check return values for CUDA calls
     cudaError_t err = cudaSuccess;
 
-    // Print the vector length to be used, and compute its size
-    int numElements = 50000;
-    size_t size = numElements * sizeof(float);
-    printf("[Vector addition of %d elements]\n", numElements);
-
-    // Allocate the host input vector A
-    float *h_A = (float *)malloc(size);
+    cudaEvent_t start_event, stop_event;
 
-    // Allocate the host input vector B
-    float *h_B = (float *)malloc(size);
+    struct timeval tv_start, tv_stop;
 
-    // Allocate the host output vector C
-    float *h_C = (float *)malloc(size);
+    double map_time = 0;       // in ms
+    double free_time = 0;       // in ms
+    float kernel_time = 0;          // in ms
 
-    // Verify that allocations succeeded
-    if (h_A == NULL || h_B == NULL || h_C == NULL)
+    if (argc != 4)
     {
-        fprintf(stderr, "Failed to allocate host vectors!\n");
-        exit(EXIT_FAILURE);
+        fprintf(stderr, "Usage: %s <vector_size> <threads_per_block> <folder>\n", argv[0]);
+        fprintf(stderr, "export DRAGON_TEST_DIRECT=1 to use D_F_DIRECT\n");
+        return EXIT_SUCCESS;
     }
 
-    // Initialize the host input vectors
-    for (int i = 0; i < numElements; ++i)
+    bool use_direct = false;
+    unsigned int map_read_flags = D_F_READ;
+
+    char *env_val = secure_getenv("DRAGON_TEST_DIRECT");
+    if (env_val && atoi(env_val) > 0)
     {
-        h_A[i] = rand()/(float)RAND_MAX;
-        h_B[i] = rand()/(float)RAND_MAX;
+        map_read_flags |= D_F_DIRECT;
+        use_direct = true;
+        fprintf(stderr, "Applying D_F_DIRECT to read-only mapping\n");
     }
 
-    // Allocate the device input vector A
-    float *d_A = NULL;
-    err = cudaMalloc((void **)&d_A, size);
+    unsigned long numElements = atol(argv[1]);
+    size_t threads_per_block = atol(argv[2]);
+    char *folder = argv[3];
 
-    if (err != cudaSuccess)
+    char *filepath = (char *)malloc(sizeof(char) * (strlen(folder) + 128));
+    if (!filepath)
     {
-        fprintf(stderr, "Failed to allocate device vector A (error code %s)!\n", cudaGetErrorString(err));
+        fprintf(stderr, "Cannot allocate filepath");
         exit(EXIT_FAILURE);
     }
 
-    // Allocate the device input vector B
-    float *d_B = NULL;
-    err = cudaMalloc((void **)&d_B, size);
+    // Print the vector length to be used, and compute its size
+    size_t size = sizeof(float) * numElements;
+    printf("[Vector addition of %llu elements]\n", numElements);
 
-    if (err != cudaSuccess)
+    float *g_A, *g_B, *g_C;
+
+    gettimeofday(&tv_start, NULL);
+    sprintf(filepath, "%s/a.mem", folder);
+    if (dragon_map(filepath, size, map_read_flags, (void **) &g_A) != D_OK)
     {
-        fprintf(stderr, "Failed to allocate device vector B (error code %s)!\n", cudaGetErrorString(err));
+        fprintf(stderr, "Cannot do dragon_map %s\n", filepath);
         exit(EXIT_FAILURE);
     }
 
-    // Allocate the device output vector C
-    float *d_C = NULL;
-    err = cudaMalloc((void **)&d_C, size);
-
-    if (err != cudaSuccess)
+    sprintf(filepath, "%s/b.mem", folder);
+    if (dragon_map(filepath, size, map_read_flags, (void **) &g_B) != D_OK)
     {
-        fprintf(stderr, "Failed to allocate device vector C (error code %s)!\n", cudaGetErrorString(err));
+        fprintf(stderr, "Cannot do dragon_map %s\n", filepath);
         exit(EXIT_FAILURE);
     }
 
-    // Copy the host input vectors A and B in host memory to the device input vectors in
-    // device memory
-    printf("Copy input data from the host memory to the CUDA device\n");
-    err = cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
-
-    if (err != cudaSuccess)
+    if (use_direct)
+        sprintf(filepath, "%s/c.nvmgpu.direct.mem", folder);
+    else
+        sprintf(filepath, "%s/c.nvmgpu.mem", folder);
+    if (dragon_map(filepath, size, D_F_CREATE | D_F_WRITE, (void **) &g_C) != D_OK)
     {
-        fprintf(stderr, "Failed to copy vector A from host to device (error code %s)!\n", cudaGetErrorString(err));
+        fprintf(stderr, "Cannot do dragon_map %s\n", filepath);
         exit(EXIT_FAILURE);
     }
+    gettimeofday(&tv_stop, NULL);
 
-    err = cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);
+    map_time = time_diff(tv_start, tv_stop);
 
-    if (err != cudaSuccess)
-    {
-        fprintf(stderr, "Failed to copy vector B from host to device (error code %s)!\n", cudaGetErrorString(err));
-        exit(EXIT_FAILURE);
-    }
+    CUDA_CALL_SAFE(cudaEventCreate(&start_event));
+    CUDA_CALL_SAFE(cudaEventCreate(&stop_event));
 
     // Launch the Vector Add CUDA Kernel
-    int threadsPerBlock = 256;
-    int blocksPerGrid =(numElements + threadsPerBlock - 1) / threadsPerBlock;
-    printf("CUDA kernel launch with %d blocks of %d threads\n", blocksPerGrid, threadsPerBlock);
-    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, numElements);
+    long blocksPerGrid = (numElements + threads_per_block - 1) / threads_per_block;
+    printf("CUDA kernel launch with %d blocks of %d threads\n", blocksPerGrid, threads_per_block);
+    CUDA_CALL_SAFE(cudaEventRecord(start_event));
+    vectorAdd<<<blocksPerGrid, threads_per_block>>>(g_A, g_B, g_C, numElements);
+    CUDA_CALL_SAFE(cudaEventRecord(stop_event));
+    CUDA_CALL_SAFE(cudaEventSynchronize(stop_event));
+    CUDA_CALL_SAFE(cudaEventElapsedTime(&kernel_time, start_event, stop_event));
     err = cudaGetLastError();
 
     if (err != cudaSuccess)
@@ -139,60 +170,31 @@
         exit(EXIT_FAILURE);
     }
 
-    // Copy the device result vector in device memory to the host result vector
-    // in host memory.
-    printf("Copy output data from the CUDA device to the host memory\n");
-    err = cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);
-
-    if (err != cudaSuccess)
-    {
-        fprintf(stderr, "Failed to copy vector C from device to host (error code %s)!\n", cudaGetErrorString(err));
-        exit(EXIT_FAILURE);
-    }
-
-    // Verify that the result vector is correct
-    for (int i = 0; i < numElements; ++i)
-    {
-        if (fabs(h_A[i] + h_B[i] - h_C[i]) > 1e-5)
-        {
-            fprintf(stderr, "Result verification failed at element %d!\n", i);
-            exit(EXIT_FAILURE);
-        }
-    }
-
-    printf("Test PASSED\n");
-
     // Free device global memory
-    err = cudaFree(d_A);
-
-    if (err != cudaSuccess)
+    gettimeofday(&tv_start, NULL);
+    if (dragon_unmap(g_A) != D_OK)
     {
-        fprintf(stderr, "Failed to free device vector A (error code %s)!\n", cudaGetErrorString(err));
+        fprintf(stderr, "Cannot do dragon_unmap g_A\n");
         exit(EXIT_FAILURE);
     }
-
-    err = cudaFree(d_B);
-
-    if (err != cudaSuccess)
+    if (dragon_unmap(g_B) != D_OK)
     {
-        fprintf(stderr, "Failed to free device vector B (error code %s)!\n", cudaGetErrorString(err));
+        fprintf(stderr, "Cannot do dragon_unmap g_B\n");
         exit(EXIT_FAILURE);
     }
-
-    err = cudaFree(d_C);
-
-    if (err != cudaSuccess)
+    if (dragon_unmap(g_C) != D_OK)
     {
-        fprintf(stderr, "Failed to free device vector C (error code %s)!\n", cudaGetErrorString(err));
+        fprintf(stderr, "Cannot do dragon_unmap g_C\n");
         exit(EXIT_FAILURE);
     }
+    gettimeofday(&tv_stop, NULL);
+
+    free_time = time_diff(tv_start, tv_stop);
 
-    // Free host memory
-    free(h_A);
-    free(h_B);
-    free(h_C);
+    free(filepath);
 
-    printf("Done\n");
+    printf("==> header: kernel_time (ms),free_time (ms),map_time (ms)\n");
+    printf("==> data: %f,%f,%f\n", kernel_time, free_time, map_time);
     return 0;
 }
 
